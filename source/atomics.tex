\rSec0[atomics]{Atomic operations library}

\rSec1[atomics.general]{General}

\pnum
This Clause describes components for fine-grained atomic access. This access is
provided via operations on atomic objects.

\pnum
The following subclauses describe atomics requirements and components for types
and operations, as summarized below.

\begin{libsumtab}{Atomics library summary}{tab:atomics.lib.summary}
\ref{atomics.order} & Order and Consistency   &
  \\
\ref{atomics.lockfree}  & Lock-free Property   &
  \\
\ref{atomics.types.generic} & Atomic Types   & \tcode{<atomic>}
  \\
\ref{atomics.types.operations}  & Operations on Atomic Types &
  \\
\ref{atomics.flag}  & Flag Type and Operations   &
  \\
\ref{atomics.fences}  & Fences   &
  \\
\end{libsumtab}

\rSec1[atomics.syn]{Header \tcode{<atomic>} synopsis}

\indexlibrary{\idxhdr{atomic}}%
\begin{codeblock}
namespace std {
  // \ref{atomics.order}, order and consistency
  enum memory_order;
  template <class T>
    T kill_dependency(T y) noexcept;

  // \ref{atomics.lockfree}, lock-free property
  #define ATOMIC_BOOL_LOCK_FREE @\unspec@
  #define ATOMIC_CHAR_LOCK_FREE @\unspec@
  #define ATOMIC_CHAR16_T_LOCK_FREE @\unspec@
  #define ATOMIC_CHAR32_T_LOCK_FREE @\unspec@
  #define ATOMIC_WCHAR_T_LOCK_FREE @\unspec@
  #define ATOMIC_SHORT_LOCK_FREE @\unspec@
  #define ATOMIC_INT_LOCK_FREE @\unspec@
  #define ATOMIC_LONG_LOCK_FREE @\unspec@
  #define ATOMIC_LLONG_LOCK_FREE @\unspec@
  #define ATOMIC_POINTER_LOCK_FREE @\unspec@

  // \ref{atomics.types.generic}, generic types
  template<class T> struct atomic;
  template<> struct atomic<@\textit{integral}@>;
  template<class T> struct atomic<T*>;

  // \ref{atomics.types.operations.general}, general operations on atomic types
  //  In the following declarations, \textit{atomic-type} is either
  //  \tcode{atomic<T>} or a named base class for \tcode{T} from
  //  Table~\ref{tab:atomics.integral} or inferred from Table~\ref{tab:atomics.typedefs} or from \tcode{bool}.
  // If it is \tcode{atomic<T>}, then the declaration is a template
  // declaration prefixed with \tcode{template <class T>}.
  bool atomic_is_lock_free(const volatile @\textit{atomic-type}@*) noexcept;
  bool atomic_is_lock_free(const @\textit{atomic-type}@*) noexcept;
  void atomic_init(volatile @\textit{atomic-type}@*, T) noexcept;
  void atomic_init(@\textit{atomic-type}@*, T) noexcept;
  void atomic_store(volatile @\textit{atomic-type}@*, T) noexcept;
  void atomic_store(@\textit{atomic-type}@*, T) noexcept;
  void atomic_store_explicit(volatile @\textit{atomic-type}@*, T, memory_order) noexcept;
  void atomic_store_explicit(@\textit{atomic-type}@*, T, memory_order) noexcept;
  T atomic_load(const volatile @\textit{atomic-type}@*) noexcept;
  T atomic_load(const @\textit{atomic-type}@*) noexcept;
  T atomic_load_explicit(const volatile @\textit{atomic-type}@*, memory_order) noexcept;
  T atomic_load_explicit(const @\textit{atomic-type}@*, memory_order) noexcept;
  T atomic_exchange(volatile @\textit{atomic-type}@*, T) noexcept;
  T atomic_exchange(@\textit{atomic-type}@*, T) noexcept;
  T atomic_exchange_explicit(volatile @\textit{atomic-type}@*, T, memory_order) noexcept;
  T atomic_exchange_explicit(@\textit{atomic-type}@*, T, memory_order) noexcept;
  bool atomic_compare_exchange_weak(volatile @\textit{atomic-type}@*, T*, T) noexcept;
  bool atomic_compare_exchange_weak(@\textit{atomic-type}@*, T*, T) noexcept;
  bool atomic_compare_exchange_strong(volatile @\textit{atomic-type}@*, T*, T) noexcept;
  bool atomic_compare_exchange_strong(@\textit{atomic-type}@*, T*, T) noexcept;
  bool atomic_compare_exchange_weak_explicit(volatile @\textit{atomic-type}@*, T*, T, 
    memory_order, memory_order) noexcept;
  bool atomic_compare_exchange_weak_explicit(@\textit{atomic-type}@*, T*, T.
    memory_order, memory_order) noexcept;
  bool atomic_compare)exchange_strong_explicit(volatile @\textit{atomic-type}@*, T*, T,
    memory_order, memory_order) noexcept;
  bool atomic_compare_exchange_strong_explicit(@\textit{atomic-type}@*, T*, T,
    memory_order, memory_order) noexcept;

  // \ref{atomics.types.operations.templ}, templated operations on atomic types
  template <class T>
    T atomic_fetch_add(volatile @atomic<T>@*, T) noexcept;
  template <class T>
    T atomic_fetch_add(@atomic<T>@*, T) noexcept;
  template <class T>
    T atomic_fetch_add_explicit(volatile atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_add_explicit(atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_sub(volatile atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_sub(atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_sub_explicit(volatile atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_sub_explicit(atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_and(volatile atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_and(atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_and_explicit(volatile atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_and_explicit(atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_or(volatile atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_or(atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_or_explicit(volatile atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_or_explicit(atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_xor(volatile atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_xor(atomic<T>*, T) noexcept;
  template <class T>
    T atomic_fetch_xor_explicit(volatile atomic<T>*, T, memory_order) noexcept;
  template <class T>
    T atomic_fetch_xor_explicit(atomic<T>*, T, memory_order) noexcept;

  // \ref{atomics.types.operations.arith}, arithmetic operations on atomic types
  // In the following declarations, \textit{atomic-integral} is either
  // \tcode{atomic<T>} or a named base class for \tcode{T} from
  // Table~\ref{tab:atomics.integral} or inferred from Table~\ref{tab:atomics.typedefs}.
  // If it is \tcode{atomic<T>}, then the declaration is a template
  // specialization declaration prefixed with \tcode{template <>}.

  @\textit{integral}@ atomic_fetch_add(volatile @\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_add(@\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_add_explicit(volatile @\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_add_explicit(@\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_sub(volatile @\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_sub(@\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_sub_explicit(volatile @\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_sub_explicit(@\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_and(volatile @\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_and(@\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_and_explicit(volatile @\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_and_explicit(@\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_or(volatile @\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_or(@\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_or_explicit(volatile @\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_or_explicit(@\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_xor(volatile @\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_xor(@\textit{atomic-integral}@*, @\textit{integral}@) noexcept;
  @\textit{integral}@ atomic_fetch_xor_explicit(volatile @\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;
  @\textit{integral}@ atomic_fetch_xor_explicit(@\textit{atomic-integral}@*, @\textit{integral}@, memory_order) noexcept;

  // \ref{atomics.types.operations.pointer}, partial specializations for pointers

  template <class T>
    T* atomic_fetch_add(volatile atomic<T*>*, ptrdiff_t) noexcept;
  template <class T>
    T* atomic_fetch_add(atomic<T*>*, ptrdiff_t) noexcept;
  template <class T>
    T* atomic_fetch_add_explicit(volatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;
  template <class T>
    T* atomic_fetch_add_explicit(atomic<T*>*, ptrdiff_t, memory_order) noexcept;
  template <class T>
    T* atomic_fetch_sub(volatile atomic<T*>*, ptrdiff_t) noexcept;
  template <class T>
    T* atomic_fetch_sub(atomic<T*>*, ptrdiff_t) noexcept;
  template <class T>
    T* atomic_fetch_sub_explicit(volatile atomic<T*>*, ptrdiff_t, memory_order) noexcept;
  template <class T>
    T* atomic_fetch_sub_explicit(atomic<T*>*, ptrdiff_t, memory_order) noexcept;

  // \ref{atomics.types.operations.req}, initialization
  #define ATOMIC_VAR_INIT(value) @\seebelow@

  // \ref{atomics.flag}, flag type and operations
  struct atomic_flag;
  bool atomic_flag_test_and_set(volatile atomic_flag*) noexcept;
  bool atomic_flag_test_and_set(atomic_flag*) noexcept;
  bool atomic_flag_test_and_set_explicit(volatile atomic_flag*, memory_order) noexcept;
  bool atomic_flag_test_and_set_explicit(atomic_flag*, memory_order) noexcept;
  void atomic_flag_clear(volatile atomic_flag*) noexcept;
  void atomic_flag_clear(atomic_flag*) noexcept;
  void atomic_flag_clear_explicit(volatile atomic_flag*, memory_order) noexcept;
  void atomic_flag_clear_explicit(atomic_flag*, memory_order) noexcept;
  #define ATOMIC_FLAG_INIT @\seebelow@

  // \ref{atomics.fences}, fences
  extern "C" void atomic_thread_fence(memory_order) noexcept;
  extern "C" void atomic_signal_fence(memory_order) noexcept;
}
\end{codeblock}

\rSec1[atomics.order]{Order and consistency}

\begin{codeblock}
namespace std {
  typedef enum memory_order {
    memory_order_relaxed, memory_order_consume, memory_order_acquire, 
    memory_order_release, memory_order_acq_rel, memory_order_seq_cst
  } memory_order;
}
\end{codeblock}

\pnum
The enumeration \tcode{memory_order} specifies the detailed regular
(non-atomic) memory synchronization order as defined in
\ref{intro.multithread} and may provide for operation ordering. Its
enumerated values and their meanings are as follows:

\begin{itemize}
\item \tcode{memory_order_relaxed}: no operation orders memory.

\item \tcode{memory_order_release}, \tcode{memory_order_acq_rel}, and
\tcode{memory_order_seq_cst}: a store operation performs a release operation on the
affected memory location.

\item \tcode{memory_order_consume}: a load operation performs a consume operation on the
affected memory location.

\item \tcode{memory_order_acquire}, \tcode{memory_order_acq_rel}, and
\tcode{memory_order_seq_cst}: a load operation performs an acquire operation on the
affected memory location.
\end{itemize}

\enternote Atomic operations specifying \tcode{memory_order_relaxed} are relaxed
with respect to memory ordering. Implementations must still guarantee that any
given atomic access to a particular atomic object be indivisible with respect
to all other atomic accesses to that object. \exitnote

\pnum
An atomic operation \term{A} that performs a release operation on an atomic
object \term{M} synchronizes with an atomic operation \term{B} that performs
an acquire operation on \term{M} and takes its value from any side effect in the
release sequence headed by \term{A}.

\pnum
There shall be a single total order \textit{S} on all \tcode{memory_order_seq_cst}
operations, consistent with the ``happens before'' order and modification orders for all
affected locations, such that each \tcode{memory_order_seq_cst} operation
\textit{B} that loads a
value from an atomic object \textit{M}
observes one of the following values:

\begin{itemize}
\item the result of the last modification \textit{A} of \textit{M} that precedes
\textit{B} in \textit{S}, if it exists, or

\item if \textit{A} exists, the result of some modification of \textit{M} in the
visible sequence of side effects with respect to \textit{B} that is not
\tcode{memory_order_seq_cst} and that does not happen before \textit{A}, or

\item if \textit{A} does not exist, the result of some modification of \textit{M}
in the visible sequence of side effects with respect to \textit{B} that is not
\tcode{memory_order_seq_cst}.
\end{itemize}

\enternote Although it is not explicitly required that \textit{S} include locks, it can
always be extended to an order that does include lock and unlock operations, since the
ordering between those is already included in the ``happens before'' ordering. \exitnote

\pnum
For an atomic operation \textit{B} that reads the value of an atomic object \textit{M},
if there is a \tcode{memory_order_seq_cst} fence \textit{X} sequenced before \textit{B},
then \textit{B} observes either the last \tcode{memory_order_seq_cst} modification of
\textit{M} preceding \textit{X} in the total order \textit{S} or a later modification of
\textit{M} in its modification order.

\pnum
For atomic operations \textit{A} and \textit{B} on an atomic object \textit{M}, where
\textit{A} modifies \textit{M} and \textit{B} takes its value, if there is a
\tcode{memory_order_seq_cst} fence \textit{X} such that \textit{A} is sequenced before
\textit{X} and \textit{B} follows \textit{X} in \textit{S}, then \textit{B} observes
either the effects of \textit{A} or a later modification of \textit{M} in its
modification order.

\pnum
For atomic operations \textit{A} and \textit{B} on an atomic object \textit{M}, where
\textit{A} modifies \textit{M} and \textit{B} takes its value, if there are
\tcode{memory_order_seq_cst} fences \textit{X} and \textit{Y} such that \textit{A} is
sequenced before \textit{X}, \textit{Y} is sequenced before \textit{B}, and \textit{X}
precedes \textit{Y} in \textit{S}, then \textit{B} observes either the effects of
\textit{A} or a later modification of \textit{M} in its modification order.

\pnum
For atomic operations \textit{A} and \textit{B} on an atomic object \textit{M}, if there
are \tcode{memory_order_seq_cst} fences \tcode{X} and \tcode{Y} such that \textit{A} is
sequenced before \textit{X}, \textit{Y} is sequenced before \textit{B}, and \textit{X}
precedes \textit{Y} in \textit{S}, then \textit{B} occurs later than \textit{A} in the
modification order of \textit{M}.

\pnum
\enternote \tcode{memory_order_seq_cst} ensures sequential consistency only for a
program that is free of data races and uses exclusively \tcode{memory_order_seq_cst}
operations. Any use of weaker ordering will invalidate this guarantee unless extreme
care is used. In particular, \tcode{memory_order_seq_cst} fences ensure a total order
only for the fences themselves. Fences cannot, in general, be used to restore sequential
consistency for atomic operations with weaker ordering specifications. \exitnote

\pnum
An atomic store shall only store a value that has been computed from constants and
program input values by a finite sequence of program evaluations, such that each
evaluation observes the values of variables as computed by the last prior assignment in
the sequence. The ordering of evaluations in this sequence shall be such that:

\begin{itemize}
\item if an evaluation \textit{B} observes a value computed by \textit{A} in a
different thread, then \textit{B} does not happen before \textit{A}, and

\item if an evaluation \textit{A} is included in the sequence, then every
evaluation that assigns to the same variable and happens before \textit{A} is
included.
\end{itemize}

\pnum
\enternote The second requirement disallows ``out-of-thin-air'' or ``speculative'' stores of atomics when relaxed atomics are used. Since unordered operations are involved, evaluations may appear in this sequence out of thread order. For example, with \tcode{x} and \tcode{y} initially zero,

\begin{codeblock}
// Thread 1:
r1 = y.load(memory_order_relaxed);
x.store(r1, memory_order_relaxed);
\end{codeblock}

\begin{codeblock}
// Thread 2:
r2 = x.load(memory_order_relaxed);
y.store(42, memory_order_relaxed);
\end{codeblock}

is allowed to produce \tcode{r1 = r2 = 42}. The sequence of evaluations justifying this consists of:

\begin{codeblock}
y.store(42, memory_order_relaxed);
r1 = y.load(memory_order_relaxed);
x.store(r1, memory_order_relaxed);
r2 = x.load(memory_order_relaxed);
\end{codeblock}

On the other hand,

\begin{codeblock}
// Thread 1:
r1 = y.load(memory_order_relaxed);
x.store(r1, memory_order_relaxed);
\end{codeblock}

\begin{codeblock}
// Thread 2:
r2 = x.load(memory_order_relaxed);
y.store(r2, memory_order_relaxed);
\end{codeblock}

may not produce \tcode{r1 = r2 = 42}, since there is no sequence of evaluations that
results in the computation of 42. In the absence of ``relaxed'' operations and
read-modify-write operations with weaker than \tcode{memory_order_acq_rel} ordering, the
second requirement has no impact.\exitnote

\pnum
\enternote The requirements do allow \tcode{r1 == r2 == 42} in the following example,
with \tcode{x} and \tcode{y} initially zero:

\begin{codeblock}
// Thread 1:
r1 = x.load(memory_order_relaxed);
if (r1 == 42) y.store(r1, memory_order_relaxed);
\end{codeblock}

\begin{codeblock}
// Thread 2:
r2 = y.load(memory_order_relaxed);
if (r2 == 42) x.store(42, memory_order_relaxed);
\end{codeblock}

However, implementations should not allow such behavior.\exitnote

\pnum
Atomic read-modify-write operations shall always read the last value
(in the modification order) written before the write associated with
the read-modify-write operation.

\pnum
Implementations should make atomic stores visible to atomic loads within a reasonable
amount of time.

\indexlibrary{\idxcode{kill_dependency}}%
\begin{itemdecl}
template <class T>
  T kill_dependency(T y) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects The argument does not carry a dependency to the return
value~(\ref{intro.multithread}).

\pnum
\returns \tcode{y}.
\end{itemdescr}


\rSec1[atomics.lockfree]{Lock-free property}

\indeximpldef{values of various \tcode{ATOMIC_..._LOCK_FREE} macros}
\begin{codeblock}
#define ATOMIC_BOOL_LOCK_FREE @\unspec@
#define ATOMIC_CHAR_LOCK_FREE @\unspec@
#define ATOMIC_CHAR16_T_LOCK_FREE @\unspec@
#define ATOMIC_CHAR32_T_LOCK_FREE @\unspec@
#define ATOMIC_WCHAR_T_LOCK_FREE @\unspec@
#define ATOMIC_SHORT_LOCK_FREE @\unspec@
#define ATOMIC_INT_LOCK_FREE @\unspec@
#define ATOMIC_LONG_LOCK_FREE @\unspec@
#define ATOMIC_LLONG_LOCK_FREE @\unspec@
#define ATOMIC_POINTER_LOCK_FREE @\unspec@
\end{codeblock}

\pnum
The \tcode{ATOMIC_..._LOCK_FREE} macros indicate the lock-free property of the
corresponding atomic types, with the signed and unsigned variants grouped
together. The properties also apply to the corresponding (partial) specializations of the
\tcode{atomic} template. A value of 0 indicates that the types are never
lock-free. A value of 1 indicates that the types are sometimes lock-free. A
value of 2 indicates that the types are always lock-free.

\pnum
The function \tcode{atomic_is_lock_free}~(\ref{atomics.types.operations})
indicates whether the object is lock-free. In any given program execution, the
result of the lock-free query shall be consistent for all pointers of the same
type.

\pnum
\enternote Operations that are lock-free should also be address-free. That is,
atomic operations on the same memory location via two different addresses will
communicate atomically. The implementation should not depend on any
per-process state. This restriction enables communication  by memory that is
mapped into a process more than once and by memory that is shared between two
processes. \exitnote

\rSec1[atomics.types.generic]{Atomic types}

\begin{codeblock}
namespace std {
  template <class T> struct atomic {
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
    void store(T, memory_order = memory_order_seq_cst) volatile noexcept;
    void store(T, memory_order = memory_order_seq_cst) noexcept;
    T load(memory_order = memory_order_seq_cst) const volatile noexcept;
    T load(memory_order = memory_order_seq_cst) const noexcept;
    operator T() const volatile noexcept;
    operator T() const noexcept;
    T exchange(T, memory_order = memory_order_seq_cst) volatile noexcept;
    T exchange(T, memory_order = memory_order_seq_cst) noexcept;
    bool compare_exchange_weak(T&, T, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(T&, T, memory_order, memory_order) noexcept;
    bool compare_exchange_strong(T&, T, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(T&, T, memory_order, memory_order) noexcept;
    bool compare_exchange_weak(T&, T, memory_order = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_weak(T&, T, memory_order = memory_order_seq_cst) noexcept;
    bool compare_exchange_strong(T&, T, memory_order = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_strong(T&, T, memory_order = memory_order_seq_cst) noexcept;

    atomic() noexcept = default;
    constexpr atomic(T) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;
    T operator=(T) volatile noexcept;
    T operator=(T) noexcept;
  };

  template <> struct atomic<@\textit{integral}@> {
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
    void store(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    void store(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    @\textit{integral}@ load(memory_order = memory_order_seq_cst) const volatile noexcept;
    @\textit{integral}@ load(memory_order = memory_order_seq_cst) const noexcept;
    operator @\textit{integral()}@ const volatile noexcept;
    operator @\textit{integral()}@ const noexcept;
    @\textit{integral}@ exchange(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    @\textit{integral}@ exchange(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    bool compare_exchange_weak(@\textit{integral}@&, @\textit{integral}@, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(@\textit{integral}@&, @\textit{integral}@, memory_order, memory_order) noexcept;
    bool compare_exchange_strong(@\textit{integral}@&, @\textit{integral}@, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(@\textit{integral}@&, @\textit{integral}@, memory_order, memory_order) noexcept;
    bool compare_exchange_weak(@\textit{integral}@&, @\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_weak(@\textit{integral}@&, @\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    bool compare_exchange_strong(@\textit{integral}@&, @\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_strong(@\textit{integral}@&, @\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    @\textit{integral}@ fetch_add(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    @\textit{integral}@ fetch_add(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    @\textit{integral}@ fetch_sub(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    @\textit{integral}@ fetch_sub(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    @\textit{integral}@ fetch_and(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    @\textit{integral}@ fetch_and(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    @\textit{integral}@ fetch_or(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    @\textit{integral}@ fetch_or(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;
    @\textit{integral}@ fetch_xor(@\textit{integral}@, memory_order = memory_order_seq_cst) volatile noexcept;
    @\textit{integral}@ fetch_xor(@\textit{integral}@, memory_order = memory_order_seq_cst) noexcept;

    atomic() noexcept = default;
    constexpr atomic(@\textit{integral}@) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;
    @\textit{integral}@ operator=(@\textit{integral}@) volatile noexcept;
    @\textit{integral}@ operator=(@\textit{integral}@) noexcept;

    @\textit{integral}@ operator++(int) volatile noexcept;
    @\textit{integral}@ operator++(int) noexcept;
    @\textit{integral}@ operator--(int) volatile noexcept;
    @\textit{integral}@ operator--(int) noexcept;
    @\textit{integral}@ operator++() volatile noexcept;
    @\textit{integral}@ operator++() noexcept;
    @\textit{integral}@ operator--() volatile noexcept;
    @\textit{integral}@ operator--() noexcept;
    @\textit{integral}@ operator+=(@\textit{integral}@) volatile noexcept;
    @\textit{integral}@ operator+=(@\textit{integral}@) noexcept;
    @\textit{integral}@ operator-=(@\textit{integral}@) volatile noexcept;
    @\textit{integral}@ operator-=(@\textit{integral}@) noexcept;
    @\textit{integral}@ operator&=(@\textit{integral}@) volatile noexcept;
    @\textit{integral}@ operator&=(@\textit{integral}@) noexcept;
    @\textit{integral}@ operator|=(@\textit{integral}@) volatile noexcept;
    @\textit{integral}@ operator|=(@\textit{integral}@) noexcept;
    @\textit{integral}@ operator^=(@\textit{integral}@) volatile noexcept;
    @\textit{integral}@ operator^=(@\textit{integral}@) noexcept;
  };

  template <class T> struct atomic<T*> {
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;
    void store(T*, memory_order = memory_order_seq_cst) volatile noexcept;
    void store(T*, memory_order = memory_order_seq_cst) noexcept;
    T* load(memory_order = memory_order_seq_cst) const volatile noexcept;
    T* load(memory_order = memory_order_seq_cst) const noexcept;
    operator T*() const volatile noexcept;
    operator T*() const noexcept;
    T* exchange(T*, memory_order = memory_order_seq_cst) volatile noexcept;
    T* exchange(T*, memory_order = memory_order_seq_cst) noexcept;
    bool compare_exchange_weak(T*&, T*, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(T*&, T*, memory_order, memory_order) noexcept;
    bool compare_exchange_strong(T*&, T*, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(T*&, T*, memory_order, memory_order) noexcept;
    bool compare_exchange_weak(T*&, T*, memory_order = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_weak(T*&, T*, memory_order = memory_order_seq_cst) noexcept;
    bool compare_exchange_strong(T*&, T*, memory_order = memory_order_seq_cst) volatile noexcept;
    bool compare_exchange_strong(T*&, T*, memory_order = memory_order_seq_cst) noexcept;
    T* fetch_add(ptrdiff_t, memory_order = memory_order_seq_cst) volatile noexcept;
    T* fetch_add(ptrdiff_t, memory_order = memory_order_seq_cst) noexcept;
    T* fetch_sub(ptrdiff_t, memory_order = memory_order_seq_cst) volatile noexcept;
    T* fetch_sub(ptrdiff_t, memory_order = memory_order_seq_cst) noexcept;

    atomic() noexcept = default;
    constexpr atomic(T*) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;
    T* operator=(T*) volatile noexcept;
    T* operator=(T*) noexcept;

    T* operator++(int) volatile noexcept;
    T* operator++(int) noexcept;
    T* operator--(int) volatile noexcept;
    T* operator--(int) noexcept;
    T* operator++() volatile noexcept;
    T* operator++() noexcept;
    T* operator--() volatile noexcept;
    T* operator--() noexcept;
    T* operator+=(ptrdiff_t) volatile noexcept;
    T* operator+=(ptrdiff_t) noexcept;
    T* operator-=(ptrdiff_t) volatile noexcept;
    T* operator-=(ptrdiff_t) noexcept;
  };
}
\end{codeblock}

\pnum
There is a generic class template \tcode{atomic<T>}. The type of the template argument
\tcode{T} shall be trivially copyable~(\ref{basic.types}). \enternote Type arguments that are
not also statically initializable may be difficult to use. \exitnote

\pnum
The semantics of the operations on specializations of \tcode{atomic} are defined
in~\ref{atomics.types.operations}.

\pnum
Specializations and instantiations of the \tcode{atomic} template shall have a deleted copy constructor, a deleted
copy assignment operator, and a constexpr value constructor.

\pnum
There shall be full specializations of the \tcode{atomic}
template for the integral types
\tcode{char},
\tcode{signed char},
\tcode{unsigned char},
\tcode{short},
\tcode{unsigned short},
\tcode{int},
\tcode{unsigned int},
\tcode{long},
\tcode{unsigned long},
\tcode{long long},
\tcode{unsigned long long},
\tcode{char16_t},
\tcode{char32_t},
\tcode{wchar_t},
and any other types needed by the typedefs in the header \tcode{<cstdint>}.
For each integral type \textit{integral}, the specialization
\tcode{atomic<integral>} provides additional atomic operations appropriate to integral types.
There shall be a specialization \tcode{atomic<bool>} which provides the general
atomic operations as specified in \ref{atomics.types.operations.general}.

\pnum
The atomic integral specializations and the specialization \tcode{atomic<bool>}
shall have standard layout. They shall each have a trivial default constructor
and a trivial destructor. They shall each support aggregate initialization
syntax.

\pnum
There shall be pointer partial specializations of the \tcode{atomic} class template.
These specializations shall have standard layout, trivial default constructors, and trivial destructors.
They shall each support aggregate initialization syntax.

\pnum
There shall be named types corresponding to the integral specializations of
\tcode{atomic}, as specified in Table~\ref{tab:atomics.integral}, and a named type
\tcode{atomic_bool} corresponding to the specified \tcode{atomic<bool>}. Each named
type is a either typedef to the corresponding specialization or a base class of the
corresponding specialization. If it is a base class, it shall support the same
member functions as the corresponding specialization.

\begin{floattablebase}
{\tcode{atomic} integral typedefs}{tab:atomics.integral}{ll}{ht}
\hline
\textbf{Named type}     & \textbf{Integral argument type} \\ \hline
\tcode{atomic_char}     & \tcode{char}                    \\
\tcode{atomic_schar}    & \tcode{signed char}             \\
\tcode{atomic_uchar}    & \tcode{unsigned char}           \\
\tcode{atomic_short}    & \tcode{short}                   \\
\tcode{atomic_ushort}   & \tcode{unsigned short}          \\
\tcode{atomic_int}      & \tcode{int}                     \\
\tcode{atomic_uint}     & \tcode{unsigned int}            \\
\tcode{atomic_long}     & \tcode{long}                    \\
\tcode{atomic_ulong}    & \tcode{unsigned long}           \\
\tcode{atomic_llong}    & \tcode{long long}               \\
\tcode{atomic_ullong}   & \tcode{unsigned long long}      \\
\tcode{atomic_char16_t} & \tcode{char16_t}                \\
\tcode{atomic_char32_t} & \tcode{char32_t}                \\
\tcode{atomic_wchar_t}  & \tcode{wchar_t}                 \\
\hline
\end{floattablebase}

\pnum
There shall be atomic typedefs corresponding to the typedefs in the header \tcode{<inttypes.h>} as
specified in Table~\ref{tab:atomics.typedefs}.

\begin{floattablebase}
{\tcode{atomic} \tcode{<inttypes.h>} typedefs}{tab:atomics.typedefs}{ll}{ht}
\hline
\textbf{Atomic typedef}   & \textbf{\tcode{<inttypes.h>} type}  \\ \hline
\tcode{atomic_int_least8_t}   & \tcode{int_least8_t}    \\
\tcode{atomic_uint_least8_t}  & \tcode{uint_least8_t}   \\
\tcode{atomic_int_least16_t}  & \tcode{int_least16_t}   \\
\tcode{atomic_uint_least16_t} & \tcode{uint_least16_t}  \\
\tcode{atomic_int_least32_t}  & \tcode{int_least32_t}   \\
\tcode{atomic_uint_least32_t} & \tcode{uint_least32_t}  \\
\tcode{atomic_int_least64_t}  & \tcode{int_least64_t}   \\
\tcode{atomic_uint_least64_t} & \tcode{uint_least64_t}  \\
\tcode{atomic_int_fast8_t}    & \tcode{int_fast8_t}     \\
\tcode{atomic_uint_fast8_t}   & \tcode{uint_fast8_t}    \\
\tcode{atomic_int_fast16_t}   & \tcode{int_fast16_t}    \\
\tcode{atomic_uint_fast16_t}  & \tcode{uint_fast16_t}   \\
\tcode{atomic_int_fast32_t}   & \tcode{int_fast32_t}    \\
\tcode{atomic_uint_fast32_t}  & \tcode{uint_fast32_t}   \\
\tcode{atomic_int_fast64_t}   & \tcode{int_fast64_t}    \\
\tcode{atomic_uint_fast64_t}  & \tcode{uint_fast64_t}   \\
\tcode{atomic_intptr_t}       & \tcode{intptr_t}        \\
\tcode{atomic_uintptr_t}      & \tcode{uintptr_t}       \\
\tcode{atomic_size_t}         & \tcode{size_t}          \\
\tcode{atomic_ptrdiff_t}      & \tcode{ptrdiff_t}       \\
\tcode{atomic_intmax_t}       & \tcode{intmax_t}        \\
\tcode{atomic_uintmax_t}      & \tcode{uintmax_t}       \\
\hline
\end{floattablebase}

\pnum
\enternote The representation of an atomic specialization need not have the same size as its
corresponding argument type. Specializations should have the same size whenever possible, as
this reduces the effort required to port existing code. \exitnote

\rSec1[atomics.types.operations]{Operations on atomic types}

\rSec2[atomics.types.operations.general]{General operations on atomic types}

\pnum
The implementation shall provide the functions and function templates identified as ``general operations
on atomic types'' in~\ref{atomics.syn}.

\pnum
In the declarations of these functions and function templates, the name
\textit{atomic-type} refers to either \tcode{atomic<T>} or to a named base class for \tcode{T}
from Table~\ref{tab:atomics.integral} or inferred from Table~\ref{tab:atomics.typedefs}.

\rSec2[atomics.types.operations.templ]{Templated operations on atomic types}

\pnum
The implementation shall declare but not define the
function templates identified as ``templated operations on atomic types'' in~\ref{atomics.syn}.

\rSec2[atomics.types.operations.arith]{Arithmetic operations on atomic types}

\pnum
The implementation shall provide the functions and function template specializations identified as ``arithmetic operations
on atomic types'' in~\ref{atomics.syn}.

\pnum
In the declarations of these functions and function template specializations, the name \textit{integral} refers to an
integral type and the name \textit{atomic-integral} refers to either
\tcode{atomic<\textit{integral}>} or to a named base class for \tcode{\textit{integral}} from
Table~\ref{tab:atomics.integral} or inferred from Table~\ref{tab:atomics.typedefs}.

\rSec2[atomics.types.operations.pointer]{Operations on atomic pointer types}

\pnum
The implementation shall provide the function template specializations
identified as ``partial specializations for pointers'' in~\ref{atomics.syn}.

\rSec2[atomics.types.operations.req]{Requirements for operations on atomic types}

\pnum
There are only a few kinds of operations on atomic types, though there are many
instances on those kinds. This section specifies each general kind. The specific
instances are defined in 
\ref{atomics.types.generic}, \ref{atomics.types.operations.general},
\ref{atomics.types.operations.arith}, and \ref{atomics.types.operations.pointer}.

\pnum
In the following operation definitions:

\begin{itemize}
\item  an \textit{A} refers to one of the atomic types.
\item a \textit{C} refers to its corresponding non-atomic type. The
\tcode{atomic_address} atomic type corresponds to the \tcode{void*} non-atomic type.
\item an \textit{M} refers to type of the other argument for arithmetic operations. For
integral atomic types, \textit{M} is \textit{C}. For atomic address types, \textit{M} is
\tcode{std::ptrdiff_t}.
\item the non member functions not ending in \tcode{_explicit} have the semantics of their
corresponding \tcode{_explicit} with \tcode{memory_order} arguments of
\tcode{memory_order_seq_cst}.
\end{itemize}

\pnum
\enternote Many operations are volatile-qualified. The ``volatile as device register''
semantics have not changed in the standard. This qualification means that volatility is
preserved when applying these operations to volatile objects. It does not mean that
operations on non-volatile objects become volatile. Thus, volatile qualified operations
on non-volatile objects may be merged under some conditions. \exitnote

\indexlibrary{\idxcode{atomic type}!constructor}%
\begin{itemdecl}
@\textit{A}@::@\textit{A}@() noexcept = default;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
leaves the atomic object in an uninitialized state.
\enternote
These semantics ensure compatibility with C.
\exitnote
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!constructor}%
\begin{itemdecl}
constexpr @\textit{A}@::@\textit{A}@(@\textit{C}@ desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects Initializes the object with the value \tcode{desired}.
Initialization is not an atomic operation~(\ref{intro.multithread}).
\enternote it is possible to have an access to an atomic object \tcode{A}
race with its construction, for example by communicating the address of the
just-constructed object \tcode{A} to another thread via
\tcode{memory_order_relaxed} operations on a suitable atomic pointer
variable, and then immediately accessing \tcode{A} in the receiving thread.
This results in undefined behavior. \exitnote
\end{itemdescr}

\begin{itemdecl}
#define ATOMIC_VAR_INIT(value) @\seebelow@
\end{itemdecl}

\begin{itemdescr}
\pnum
The macro expands to a token sequence suitable for
constant initialization of
an atomic variable of static storage duration of a type that is
initialization-compatible with \textit{value}.
\enternote This operation may need to initialize locks. \exitnote
Concurrent access to the variable being initialized, even via an atomic operation,
constitutes a data race. \enterexample
\begin{codeblock}
atomic<int> v = ATOMIC_VAR_INIT(5);
\end{codeblock}
\exitexample
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_is_lock_free}}%
\indexlibrary{\idxcode{atomic_is_lock_free}!\idxcode{atomic type}}%
\begin{itemdecl}
bool atomic_is_lock_free(const volatile @\textit{A}@ *object) noexcept;
bool atomic_is_lock_free(const @\textit{A}@ *object) noexcept;
bool @\textit{A}@::is_lock_free() const volatile noexcept;
bool @\textit{A}@::is_lock_free() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\returns True if the object's operations are lock-free, false otherwise.
\end{itemdescr}

\begin{itemdecl}
void atomic_init(volatile @\textit{A}@ *object, @\textit{C}@ desired) noexcept;
void atomic_init(@\textit{A}@ *object, @\textit{C}@ desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects Non-atomically
initializes \tcode{*object} with value \tcode{desired}. This function shall only be applied
to objects that have been default constructed, and then only once.
\enternote
These semantics ensure compatibility with C.
\exitnote
\enternote 
Concurrent access from another thread, even via an atomic operation, constitutes
a data race.
\exitnote

\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_store}}%
\indexlibrary{\idxcode{atomic_store}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_store_explicit}}%
\indexlibrary{\idxcode{atomic_store_explicit}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{store}}%
\indexlibrary{\idxcode{store}!\idxcode{atomic type}}%
\begin{itemdecl}
void atomic_store(volatile @\textit{A}@* object, @\textit{C}@ desired) noexcept;
void atomic_store(@\textit{A}@* object, @\textit{C}@ desired) noexcept;
void atomic_store_explicit(volatile @\textit{A}@ *object, @\textit{C}@ desired, memory_order order) noexcept;
void atomic_store_explicit(@\textit{A}@* object, @\textit{C}@ desired, memory_order order) noexcept;
void @\textit{A}@::store(@\textit{C}@ desired, memory_order order = memory_order_seq_cst) volatile noexcept;
void @\textit{A}@::store(@\textit{C}@ desired, memory_order order = memory_order_seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\requires The \tcode{order} argument shall not be \tcode{memory_order_consume},
\tcode{memory_order_acquire}, nor \tcode{memory_order_acq_rel}.

\pnum
\effects Atomically replaces the value pointed to by \tcode{object} or by \tcode{this}
with the value of \tcode{desired}. Memory is affected according to the value of
\tcode{order}.
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{operator=}}%
\indexlibrary{\idxcode{operator=}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C} \textit{A}@::operator=(@\textit{C}@ desired) volatile noexcept;
@\textit{C}@ @\textit{A}@::operator=(@\textit{C}@ desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \tcode{store(desired)}

\pnum
\returns \tcode{desired}
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_load}}%
\indexlibrary{\idxcode{atomic_load}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_load_explicit}}%
\indexlibrary{\idxcode{atomic_load_explicit}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{load}}%
\indexlibrary{\idxcode{load}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ atomic_load(const volatile @\textit{A}@* object) noexcept;
@\textit{C}@ atomic_load(const @\textit{A}@* object) noexcept;
@\textit{C}@ atomic_load_explicit(const volatile @\textit{A}@* object, memory_order) noexcept;
@\textit{C}@ atomic_load_explicit(const @\textit{A}@* object, memory_order) noexcept;
@\textit{C}@ @\textit{A}@::load(memory_order order = memory_order_seq_cst) const volatile noexcept;
@\textit{C}@ @\textit{A}@::load(memory_order order = memory_order_seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\requires The \tcode{order} argument shall not be \tcode{memory_order_release} nor \tcode{memory_order_acq_rel}.

\pnum
\effects Memory is affected according to the value of \tcode{order}.

\pnum
\returns Atomically returns the value pointed to by \tcode{object} or by \tcode{this}.
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!operator C@\tcode{operator \textit{C}}}%
\indexlibrary{operator C@\tcode{operator \textit{C}}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{A}@::operator @\textit{C}@() const volatile noexcept;
@\textit{A}@::operator @\textit{C}@() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \tcode{load()}

\pnum
\returns The result of \tcode{load()}.
\end{itemdescr}


\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_exchange}}%
\indexlibrary{\idxcode{atomic_exchange}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_exchange_explicit}}%
\indexlibrary{\idxcode{atomic_exchange_explicit}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{exchange}}%
\indexlibrary{\idxcode{exchange}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ atomic_exchange(volatile @\textit{A}@* object, @\textit{C}@ desired) noexcept;
@\textit{C}@ atomic_exchange(@\textit{A}@* object, @\textit{C}@ desired) noexcept;
@\textit{C}@ atomic_exchange_explicit(volatile @\textit{A}@* object, @\textit{C}@ desired, memory_order) noexcept;
@\textit{C}@ atomic_exchange_explicit(@\textit{A}@* object, @\textit{C}@ desired, memory_order) noexcept;
@\textit{C}@ @\textit{A}@::exchange(@\textit{C}@ desired, memory_order order = memory_order_seq_cst) volatile noexcept;
@\textit{C}@ @\textit{A}@::exchange(@\textit{C}@ desired, memory_order order = memory_order_seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects Atomically replaces the value pointed to by \tcode{object} or by \tcode{this}
with \tcode{desired}.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations~(\ref{intro.multithread}).

\pnum
\returns Atomically returns the value pointed to by \tcode{object} or by \tcode{this} immediately before the effects.
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_compare_exchange_weak}}%
\indexlibrary{\idxcode{atomic_compare_exchange_weak}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_compare_exchange_strong}}%
\indexlibrary{\idxcode{atomic_compare_exchange_strong}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!atomic_compare_exchange_weak_explicit@\tcode{atomic_compare_exchange_weak_-\\explicit}}%
\indexlibrary{\idxcode{atomic_compare_exchange_weak_explicit}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!atomic_compare_exchange_strong_explicit@\tcode{atomic_compare_exchange_strong_-\\explicit}}%
\indexlibrary{\idxcode{atomic_compare_exchange_strong_explicit}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{compare_exchange_weak}}%
\indexlibrary{\idxcode{compare_exchange_weak}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{compare_exchange_strong}}%
\indexlibrary{\idxcode{compare_exchange_strong}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{compare_exchange_weak_explicit}}%
\indexlibrary{\idxcode{compare_exchange_weak_explicit}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{compare_exchange_strong_explicit}}%
\indexlibrary{\idxcode{compare_exchange_strong_explicit}!\idxcode{atomic type}}%
\begin{itemdecl}
bool atomic_compare_exchange_weak(volatile @\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired) noexcept;
bool atomic_compare_exchange_weak(@\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired) noexcept;
bool atomic_compare_exchange_strong(volatile @\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired) noexcept;
bool atomic_compare_exchange_strong(@\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired) noexcept;
bool atomic_compare_exchange_weak_explicit(volatile @\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) noexcept;
bool atomic_compare_exchange_weak_explicit(@\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) noexcept;
bool atomic_compare_exchange_strong_explicit(volatile @\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) noexcept;
bool atomic_compare_exchange_strong_explicit(@\textit{A}@* object, @\textit{C}@* expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) noexcept;
bool @\textit{A}@::compare_exchange_weak(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) volatile noexcept;
bool @\textit{A}@::compare_exchange_weak(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) noexcept;
bool @\textit{A}@::compare_exchange_strong(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) volatile noexcept;
bool @\textit{A}@::compare_exchange_strong(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order success, memory_order failure) noexcept;
bool @\textit{A}@::compare_exchange_weak(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order order = memory_order_seq_cst) volatile noexcept;
bool @\textit{A}@::compare_exchange_weak(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order order = memory_order_seq_cst) noexcept;
bool @\textit{A}@::compare_exchange_strong(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order order = memory_order_seq_cst) volatile noexcept;
bool @\textit{A}@::compare_exchange_strong(@\textit{C}@& expected, @\textit{C}@ desired,
    memory_order order = memory_order_seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\requires The \tcode{failure} argument shall not be \tcode{memory_order_release} nor
\tcode{memory_order_acq_rel}. The \tcode{failure} argument shall be no stronger than the
\tcode{success} argument.

\pnum
\effects Atomically, compares the contents of the memory pointed to by \tcode{object} or by \tcode{this}
for equality with that in \tcode{expected}, and if true, replaces the contents of the memory pointed to
by \tcode{object} or by \tcode{this} with that in \tcode{desired}, and if false, updates the
contents of the memory in \tcode{expected} with the contents of the memory pointed to by \tcode{object} or by
\tcode{this}. Further, if the comparison is true, memory is affected according to the
value of \tcode{success}, and if the comparison is false, memory is affected according
to the value of \tcode{failure}. When only one \tcode{memory_order} argument is
supplied, the value of \tcode{success} is \tcode{order}, and the value of
\tcode{failure} is \tcode{order} except that a value of \tcode{memory_order_acq_rel}
shall be replaced by the value \tcode{memory_order_acquire} and a value of
\tcode{memory_order_release} shall be replaced by the value
\tcode{memory_order_relaxed}. If the operation returns \tcode{true}, these
operations are atomic read-modify-write
operations~(\ref{intro.multithread}). Otherwise, these operations are atomic load operations.

\pnum
\returns The result of the comparison.

\pnum
\enternote For example, the effect of
\tcode{atomic_compare_exchange_strong} is
\begin{codeblock}
if (memcmp(object, expected, sizeof(*object)) == 0)
  memcpy(object, &desired, sizeof(*object));
else
  memcpy(expected, object, sizeof(*object));
\end{codeblock}
\exitnote
\enterexample the expected use of the compare-and-exchange operations is as follows. The
compare-and-exchange operations will update \tcode{expected} when another iteration of
the loop is needed.
\begin{codeblock}
expected = current.load();
do {
  desired = function(expected);
} while (!current.compare_exchange_weak(expected, desired));
\end{codeblock}
\exitexample

\pnum
Implementations should ensure that weak compare-and-exchange operations do not
consistently return \tcode{false} unless either the atomic object has value
different from \tcode{expected} or there are concurrent modifications to the
atomic object.

\pnum
\note
A weak compare-and-exchange operation may fail spuriously. That is, even when
the contents of memory referred to by \tcode{expected} and \tcode{object} are
equal, it may return false and store back to \tcode{expected} the same memory
contents that were originally there.
\enternote This
spurious failure enables implementation of compare-and-exchange on a broader class of
machines, e.g., load-locked store-conditional machines. A
consequence of spurious failure is that nearly all uses of weak compare-and-exchange
will be in a loop.

When a compare-and-exchange is in a loop, the weak version will yield better performance
on some platforms. When a weak compare-and-exchange would require a loop and a strong one
would not, the strong one is preferable.
\exitnote

\pnum
\enternote The \tcode{memcpy} and \tcode{memcmp} semantics of the compare-and-exchange
operations may result in failed comparisons for values that compare equal with
\tcode{operator==} if the underlying type has padding bits, trap bits, or alternate
representations of the same value. Thus, \tcode{compare_exchange_strong} should be used
with extreme care. On the other hand, \tcode{compare_exchange_weak} should converge
rapidly. \exitnote
\end{itemdescr}

\pnum
The following operations perform arithmetic computations. The key, operator, and computation correspondence is:

\begin{floattable}
{Atomic arithmetic computations}{tab:atomic.arithmetic.computations}{lll|lll}
\hline
\tcode{Key}       &
  Op          &
  Computation     &
\tcode{Key}       &
  Op          &
  Computation     \\ \hline
\tcode{add}       &
  \tcode{+}       &
  addition        &
\tcode{sub}       &
  \tcode{-}       &
  subtraction     \\
\tcode{or}        &
  \tcode{|}       &
  bitwise inclusive or  &
\tcode{xor}       &
  \tcode{\^{}}        &
  bitwise exclusive or  \\
\tcode{and}       &
  \tcode{\&}      &
  bitwise and     &&&\\\hline
\end{floattable}

\indexlibrary{\idxcode{atomic type}!\idxcode{atomic_fetch_}}%
\indexlibrary{\idxcode{atomic_fetch_}!\idxcode{atomic type}}%
\indexlibrary{\idxcode{atomic type}!\idxcode{fetch_}}%
\indexlibrary{\idxcode{fetch_}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ atomic_fetch_@\textit{key}@(volatile @\textit{A}@ *object, @\textit{M}@ operand) noexcept;
@\textit{C}@ atomic_fetch_@\textit{key}@(@\textit{A}@* object, @\textit{M}@ operand) noexcept;
@\textit{C}@ atomic_fetch_@\textit{key}@_explicit(volatile @\textit{A}@ *object, @\textit{M}@ operand, memory_order order) noexcept;
@\textit{C}@ atomic_fetch_@\textit{key}@_explicit(@\textit{A}@* object, @\textit{M}@ operand, memory_order order) noexcept;
@\textit{C}@ @\textit{A}@::fetch_@\textit{key}@(@\textit{M}@ operand, memory_order order = memory_order_seq_cst) volatile noexcept;
@\textit{C}@ @\textit{A}@::fetch_@\textit{key}@(@\textit{M}@ operand, memory_order order = memory_order_seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects Atomically replaces the value pointed to by \tcode{object} or by
\tcode{this} with the result of the \textit{computation} applied to the
value pointed to by \tcode{object} or by \tcode{this} and the given \tcode{operand}.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations~(\ref{intro.multithread}).

\pnum
\returns Atomically, the value pointed to by \tcode{object} or by \tcode{this} immediately before the effects.

\pnum
\note For signed integer types, arithmetic is defined to use two's complement
representation. There are no undefined results. For address types, the result may be an
undefined address, but the operations otherwise have no undefined behavior.
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{operator "@=}}%
\indexlibrary{\idxcode{operator "@=}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ @\textit{A}@::operator @\textit{op}@=(@\textit{M}@ operand) volatile noexcept;
@\textit{C}@ @\textit{A}@::operator @\textit{op}@=(@\textit{M}@ operand) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \tcode{fetch_\textit{key}(operand)}

\pnum
\returns \tcode{fetch_\textit{key}(operand) op operand}
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{operator++}}%
\indexlibrary{\idxcode{operator++}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ @\textit{A}@::operator++(int) volatile noexcept;
@\textit{C}@ @\textit{A}@::operator++(int) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\returns \tcode{fetch_add(1)}
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{operator\dcr}}%
\indexlibrary{\idxcode{operator\dcr}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ @\textit{A}@::operator--(int) volatile noexcept;
@\textit{C}@ @\textit{A}@::operator--(int) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\returns \tcode{fetch_sub(1)}
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{operator++}}%
\indexlibrary{\idxcode{operator++}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ @\textit{A}@::operator++() volatile noexcept;
@\textit{C}@ @\textit{A}@::operator++() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \tcode{fetch_add(1)}

\pnum
\returns \tcode{fetch_add(1) + 1}
\end{itemdescr}

\indexlibrary{\idxcode{atomic type}!\idxcode{operator\dcr}}%
\indexlibrary{\idxcode{operator\dcr}!\idxcode{atomic type}}%
\begin{itemdecl}
@\textit{C}@ @\textit{A}@::operator--() volatile noexcept;
@\textit{C}@ @\textit{A}@::operator--() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects \tcode{fetch_sub(1)}

\pnum
\returns \tcode{fetch_sub(1) - 1}
\end{itemdescr}

\rSec1[atomics.flag]{Flag type and operations}

\begin{codeblock}
namespace std {
  typedef struct atomic_flag {
    bool test_and_set(memory_order = memory_order_seq_cst) volatile noexcept;
    bool test_and_set(memory_order = memory_order_seq_cst) noexcept;
    void clear(memory_order = memory_order_seq_cst) volatile noexcept;
    void clear(memory_order = memory_order_seq_cst) noexcept;

    atomic_flag() noexcept = default;
    atomic_flag(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) volatile = delete;
  } atomic_flag;

  bool atomic_flag_test_and_set(volatile atomic_flag*) noexcept;
  bool atomic_flag_test_and_set(atomic_flag*) noexcept;
  bool atomic_flag_test_and_set_explicit(volatile atomic_flag*, memory_order) noexcept;
  bool atomic_flag_test_and_set_explicit(atomic_flag*, memory_order) noexcept;
  void atomic_flag_clear(volatile atomic_flag*) noexcept;
  void atomic_flag_clear(atomic_flag*) noexcept;
  void atomic_flag_clear_explicit(volatile atomic_flag*, memory_order) noexcept;
  void atomic_flag_clear_explicit(atomic_flag*, memory_order) noexcept;

  #define ATOMIC_FLAG_INIT @\seebelow@
}
\end{codeblock}

\pnum
The \tcode{atomic_flag} type provides the classic test-and-set functionality. It has two states, set and clear.

\pnum
Operations on an object of type \tcode{atomic_flag} shall be lock-free. \enternote Hence
the operations should also be address-free. No other type requires lock-free operations,
so the \tcode{atomic_flag} type is the minimum hardware-implemented type needed to
conform to this International standard. The remaining types can be emulated with
\tcode{atomic_flag}, though with less than ideal properties. \exitnote

\pnum
The \tcode{atomic_flag} type shall have standard layout. It shall have a trivial default constructor, a deleted copy constructor, a deleted copy assignment operator, and a trivial destructor.

\pnum
The macro \tcode{ATOMIC_FLAG_INIT} shall be defined in such a way that it can be used to initialize an object of type \tcode{atomic_flag} to the
clear state. For a static-duration object, that initialization shall be static. It is unspecified whether an
uninitialized \tcode{atomic_flag} object has an initial state of set or clear.\enterexample

\begin{codeblock}
atomic_flag guard = ATOMIC_FLAG_INIT;
\end{codeblock}
\exitexample

\indexlibrary{\idxcode{atomic_flag_test_and_set}}%
\indexlibrary{\idxcode{atomic_flag_test_and_set_explicit}}%
\begin{itemdecl}
bool atomic_flag_test_and_set(volatile atomic_flag *object) noexcept;
bool atomic_flag_test_and_set(atomic_flag *object) noexcept;
bool atomic_flag_test_and_set_explicit(volatile atomic_flag *object, memory_order order) noexcept;
bool atomic_flag_test_and_set_explicit(atomic_flag *object, memory_order order) noexcept;
bool atomic_flag::test_and_set(memory_order order = memory_order_seq_cst) volatile noexcept;
bool atomic_flag::test_and_set(memory_order order = memory_order_seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects Atomically sets the value pointed to by \tcode{object} or by \tcode{this} to true. Memory is affected according to the value of
\tcode{order}. These operations are atomic read-modify-write operations~(\ref{intro.multithread}).

\pnum
\returns Atomically, the value of the object immediately before the effects. \end{itemdescr}

\indexlibrary{\idxcode{atomic_flag_clear}}%
\indexlibrary{\idxcode{atomic_flag_clear_explicit}}%
\indexlibrary{\idxcode{atomic_flag}!\idxcode{clear}}%
\indexlibrary{\idxcode{clear}!\idxcode{atomic_flag}}%
\begin{itemdecl}
void atomic_flag_clear(volatile atomic_flag *object) noexcept;
void atomic_flag_clear(atomic_flag *object) noexcept;
void atomic_flag_clear_explicit(volatile atomic_flag *object, memory_order order) noexcept;
void atomic_flag_clear_explicit(atomic_flag *object, memory_order order) noexcept;
void atomic_flag::clear(memory_order order = memory_order_seq_cst) volatile noexcept;
void atomic_flag::clear(memory_order order = memory_order_seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\requires The \tcode{order} argument shall not be \tcode{memory_order_acquire}
 or \tcode{memory_order_acq_rel}.

\pnum
\effects Atomically sets the value pointed to by \tcode{object} or by \tcode{this} to
false. Memory is affected according to the value of \tcode{order}.
\end{itemdescr}

\rSec1[atomics.fences]{Fences}

\pnum
This section introduces synchronization primitives called \term{fences}. Fences can have
acquire semantics, release semantics, or both. A fence with acquire semantics is called
an \term{acquire fence}. A fence with release semantics is called a \term{release
fence}.

\pnum
A release fence \textit{A} synchronizes with an acquire fence \textit{B} if there exist
atomic operations \textit{X} and \textit{Y}, both operating on some atomic object
\textit{M}, such that \textit{A} is sequenced before \textit{X}, \textit{X} modifies
\textit{M}, \textit{Y} is sequenced before \textit{B}, and \textit{Y} reads the value
written by \textit{X} or a value written by any side effect in the hypothetical release
sequence \textit{X} would head if it were a release operation.

\pnum
A release fence \textit{A} synchronizes with an atomic operation \textit{B} that
performs an acquire operation on an atomic object \textit{M} if there exists an atomic
operation \textit{X} such that \textit{A} is sequenced before \textit{X}, \textit{X}
modifies \textit{M}, and \textit{B} reads the value written by \textit{X} or a value
written by any side effect in the hypothetical release sequence \textit{X} would head if
it were a release operation.

\pnum
An atomic operation \textit{A} that is a release operation on an atomic object
\textit{M} synchronizes with an acquire fence \textit{B} if there exists some atomic
operation \textit{X} on \textit{M} such that \textit{X} is sequenced before \textit{B}
and reads the value written by \textit{A} or a value written by any side effect in the
release sequence headed by \textit{A}.

\indexlibrary{\idxcode{atomic_thread_fence}}%
\begin{itemdecl}
extern "C" void atomic_thread_fence(memory_order order) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects depending on the value of \tcode{order}, this operation:

\begin{itemize}
\item has no effects, if \tcode{order == memory_order_relaxed};

\item is an acquire fence, if \tcode{order == memory_order_acquire || order ==
memory_order_consume};

\item is a release fence, if \tcode{order == memory_order_release};

\item is both an acquire fence and a release fence, if \tcode{order ==
memory_order_acq_rel};

\item is a sequentially consistent acquire and release fence, if \tcode{order == memory_order_seq_cst}.
\end{itemize}
\end{itemdescr}

\indexlibrary{\idxcode{atomic_signal_fence}}%
\begin{itemdecl}
extern "C" void atomic_signal_fence(memory_order order) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects equivalent to \tcode{atomic_thread_fence(order)}, except that
the resulting ordering constraints are established only between a thread and a
signal handler executed in the same thread.

\pnum
\realnote \tcode{atomic_signal_fence} can be used to specify the order in which actions
performed by the thread become visible to the signal handler.

\pnum
\realnote compiler optimizations and reorderings of loads and stores are inhibited in
the same way as with \tcode{atomic_thread_fence}, but the hardware fence instructions
that \tcode{atomic_thread_fence} would have inserted are not emitted.
\end{itemdescr}

%!TEX root = std.tex
\rSec0[atomics]{Atomic operations library}

\rSec1[atomics.general]{General}

\pnum
This Clause describes components for fine-grained atomic access. This access is
provided via operations on atomic objects.

\pnum
The following subclauses describe atomics requirements and components for types
and operations, as summarized in \tref{atomics.summary}.

\begin{libsumtab}{Atomics library summary}{atomics.summary}
\ref{atomics.alias} & Type aliases  & \tcode{<atomic>} \\
\ref{atomics.order} & Order and consistency   &  \\
\ref{atomics.lockfree}  & Lock-free property   &  \\
\ref{atomics.wait}  & Waiting and notifying  &  \\
\ref{atomics.ref.generic} & Class template \tcode{atomic_ref} &  \\
\ref{atomics.types.generic} & Class template \tcode{atomic}   &  \\
\ref{atomics.nonmembers}  & Non-member functions &  \\
\ref{atomics.flag}  & Flag type and operations   &  \\
\ref{atomics.fences}  & Fences   &  \\ \rowsep
\ref{stdatomic.h.syn}  & C compatibility   & \tcode{<stdatomic.h>} \\
\end{libsumtab}

\rSec1[atomics.syn]{Header \tcode{<atomic>} synopsis}

\indexheader{atomic}%
\begin{codeblock}
namespace std {
  // \ref{atomics.order}, order and consistency
  enum class memory_order : @\unspec@;
  template<class T>
    T kill_dependency(T y) noexcept;
}

// \ref{atomics.lockfree}, lock-free property
#define ATOMIC_BOOL_LOCK_FREE @\unspec@
#define ATOMIC_CHAR_LOCK_FREE @\unspec@
#define ATOMIC_CHAR8_T_LOCK_FREE @\unspec@
#define ATOMIC_CHAR16_T_LOCK_FREE @\unspec@
#define ATOMIC_CHAR32_T_LOCK_FREE @\unspec@
#define ATOMIC_WCHAR_T_LOCK_FREE @\unspec@
#define ATOMIC_SHORT_LOCK_FREE @\unspec@
#define ATOMIC_INT_LOCK_FREE @\unspec@
#define ATOMIC_LONG_LOCK_FREE @\unspec@
#define ATOMIC_LLONG_LOCK_FREE @\unspec@
#define ATOMIC_POINTER_LOCK_FREE @\unspec@

namespace std {
  // \ref{atomics.ref.generic}, class template \tcode{atomic_ref}
  template<class T> struct atomic_ref;
  // \ref{atomics.ref.pointer}, partial specialization for pointers
  template<class T> struct atomic_ref<T*>;

  // \ref{atomics.types.generic}, class template \tcode{atomic}
  template<class T> struct atomic;
  // \ref{atomics.types.pointer}, partial specialization for pointers
  template<class T> struct atomic<T*>;

  // \ref{atomics.nonmembers}, non-member functions
  template<class T>
    bool atomic_is_lock_free(const volatile atomic<T>*) noexcept;
  template<class T>
    bool atomic_is_lock_free(const atomic<T>*) noexcept;
  template<class T>
    void atomic_store(volatile atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    void atomic_store(atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    void atomic_store_explicit(volatile atomic<T>*, typename atomic<T>::value_type,
                               memory_order) noexcept;
  template<class T>
    void atomic_store_explicit(atomic<T>*, typename atomic<T>::value_type,
                               memory_order) noexcept;
  template<class T>
    T atomic_load(const volatile atomic<T>*) noexcept;
  template<class T>
    T atomic_load(const atomic<T>*) noexcept;
  template<class T>
    T atomic_load_explicit(const volatile atomic<T>*, memory_order) noexcept;
  template<class T>
    T atomic_load_explicit(const atomic<T>*, memory_order) noexcept;
  template<class T>
    T atomic_exchange(volatile atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_exchange(atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_exchange_explicit(volatile atomic<T>*, typename atomic<T>::value_type,
                               memory_order) noexcept;
  template<class T>
    T atomic_exchange_explicit(atomic<T>*, typename atomic<T>::value_type,
                               memory_order) noexcept;
  template<class T>
    bool atomic_compare_exchange_weak(volatile atomic<T>*,
                                      typename atomic<T>::value_type*,
                                      typename atomic<T>::value_type) noexcept;
  template<class T>
    bool atomic_compare_exchange_weak(atomic<T>*,
                                      typename atomic<T>::value_type*,
                                      typename atomic<T>::value_type) noexcept;
  template<class T>
    bool atomic_compare_exchange_strong(volatile atomic<T>*,
                                        typename atomic<T>::value_type*,
                                        typename atomic<T>::value_type) noexcept;
  template<class T>
    bool atomic_compare_exchange_strong(atomic<T>*,
                                        typename atomic<T>::value_type*,
                                        typename atomic<T>::value_type) noexcept;
  template<class T>
    bool atomic_compare_exchange_weak_explicit(volatile atomic<T>*,
                                               typename atomic<T>::value_type*,
                                               typename atomic<T>::value_type,
                                               memory_order, memory_order) noexcept;
  template<class T>
    bool atomic_compare_exchange_weak_explicit(atomic<T>*,
                                               typename atomic<T>::value_type*,
                                               typename atomic<T>::value_type,
                                               memory_order, memory_order) noexcept;
  template<class T>
    bool atomic_compare_exchange_strong_explicit(volatile atomic<T>*,
                                                 typename atomic<T>::value_type*,
                                                 typename atomic<T>::value_type,
                                                 memory_order, memory_order) noexcept;
  template<class T>
    bool atomic_compare_exchange_strong_explicit(atomic<T>*,
                                                 typename atomic<T>::value_type*,
                                                 typename atomic<T>::value_type,
                                                 memory_order, memory_order) noexcept;

  template<class T>
    T atomic_fetch_add(volatile atomic<T>*, typename atomic<T>::difference_type) noexcept;
  template<class T>
    T atomic_fetch_add(atomic<T>*, typename atomic<T>::difference_type) noexcept;
  template<class T>
    T atomic_fetch_add_explicit(volatile atomic<T>*, typename atomic<T>::difference_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_add_explicit(atomic<T>*, typename atomic<T>::difference_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_sub(volatile atomic<T>*, typename atomic<T>::difference_type) noexcept;
  template<class T>
    T atomic_fetch_sub(atomic<T>*, typename atomic<T>::difference_type) noexcept;
  template<class T>
    T atomic_fetch_sub_explicit(volatile atomic<T>*, typename atomic<T>::difference_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_sub_explicit(atomic<T>*, typename atomic<T>::difference_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_and(volatile atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_fetch_and(atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_fetch_and_explicit(volatile atomic<T>*, typename atomic<T>::value_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_and_explicit(atomic<T>*, typename atomic<T>::value_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_or(volatile atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_fetch_or(atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_fetch_or_explicit(volatile atomic<T>*, typename atomic<T>::value_type,
                               memory_order) noexcept;
  template<class T>
    T atomic_fetch_or_explicit(atomic<T>*, typename atomic<T>::value_type,
                               memory_order) noexcept;
  template<class T>
    T atomic_fetch_xor(volatile atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_fetch_xor(atomic<T>*, typename atomic<T>::value_type) noexcept;
  template<class T>
    T atomic_fetch_xor_explicit(volatile atomic<T>*, typename atomic<T>::value_type,
                                memory_order) noexcept;
  template<class T>
    T atomic_fetch_xor_explicit(atomic<T>*, typename atomic<T>::value_type,
                                memory_order) noexcept;

  template<class T>
    void atomic_wait(const volatile atomic<T>*, typename atomic<T>::value_type);
  template<class T>
    void atomic_wait(const atomic<T>*, typename atomic<T>::value_type);
  template<class T>
    void atomic_wait_explicit(const volatile atomic<T>*, typename atomic<T>::value_type,
                              memory_order);
  template<class T>
    void atomic_wait_explicit(const atomic<T>*, typename atomic<T>::value_type,
                              memory_order);
  template<class T>
    void atomic_notify_one(volatile atomic<T>*);
  template<class T>
    void atomic_notify_one(atomic<T>*);
  template<class T>
    void atomic_notify_all(volatile atomic<T>*);
  template<class T>
    void atomic_notify_all(atomic<T>*);

  // \ref{atomics.alias}, type aliases
  using atomic_bool           = atomic<bool>;
  using atomic_char           = atomic<char>;
  using atomic_schar          = atomic<signed char>;
  using atomic_uchar          = atomic<unsigned char>;
  using atomic_short          = atomic<short>;
  using atomic_ushort         = atomic<unsigned short>;
  using atomic_int            = atomic<int>;
  using atomic_uint           = atomic<unsigned int>;
  using atomic_long           = atomic<long>;
  using atomic_ulong          = atomic<unsigned long>;
  using atomic_llong          = atomic<long long>;
  using atomic_ullong         = atomic<unsigned long long>;
  using atomic_char8_t        = atomic<char8_t>;
  using atomic_char16_t       = atomic<char16_t>;
  using atomic_char32_t       = atomic<char32_t>;
  using atomic_wchar_t        = atomic<wchar_t>;

  using atomic_int8_t         = atomic<int8_t>;
  using atomic_uint8_t        = atomic<uint8_t>;
  using atomic_int16_t        = atomic<int16_t>;
  using atomic_uint16_t       = atomic<uint16_t>;
  using atomic_int32_t        = atomic<int32_t>;
  using atomic_uint32_t       = atomic<uint32_t>;
  using atomic_int64_t        = atomic<int64_t>;
  using atomic_uint64_t       = atomic<uint64_t>;

  using atomic_int_least8_t   = atomic<int_least8_t>;
  using atomic_uint_least8_t  = atomic<uint_least8_t>;
  using atomic_int_least16_t  = atomic<int_least16_t>;
  using atomic_uint_least16_t = atomic<uint_least16_t>;
  using atomic_int_least32_t  = atomic<int_least32_t>;
  using atomic_uint_least32_t = atomic<uint_least32_t>;
  using atomic_int_least64_t  = atomic<int_least64_t>;
  using atomic_uint_least64_t = atomic<uint_least64_t>;

  using atomic_int_fast8_t    = atomic<int_fast8_t>;
  using atomic_uint_fast8_t   = atomic<uint_fast8_t>;
  using atomic_int_fast16_t   = atomic<int_fast16_t>;
  using atomic_uint_fast16_t  = atomic<uint_fast16_t>;
  using atomic_int_fast32_t   = atomic<int_fast32_t>;
  using atomic_uint_fast32_t  = atomic<uint_fast32_t>;
  using atomic_int_fast64_t   = atomic<int_fast64_t>;
  using atomic_uint_fast64_t  = atomic<uint_fast64_t>;

  using atomic_intptr_t       = atomic<intptr_t>;
  using atomic_uintptr_t      = atomic<uintptr_t>;
  using atomic_size_t         = atomic<size_t>;
  using atomic_ptrdiff_t      = atomic<ptrdiff_t>;
  using atomic_intmax_t       = atomic<intmax_t>;
  using atomic_uintmax_t      = atomic<uintmax_t>;

  using atomic_signed_lock_free   = @\seebelow@;
  using atomic_unsigned_lock_free = @\seebelow@;

  // \ref{atomics.flag}, flag type and operations
  struct atomic_flag;

  bool atomic_flag_test(const volatile atomic_flag*) noexcept;
  bool atomic_flag_test(const atomic_flag*) noexcept;
  bool atomic_flag_test_explicit(const volatile atomic_flag*, memory_order) noexcept;
  bool atomic_flag_test_explicit(const atomic_flag*, memory_order) noexcept;
  bool atomic_flag_test_and_set(volatile atomic_flag*) noexcept;
  bool atomic_flag_test_and_set(atomic_flag*) noexcept;
  bool atomic_flag_test_and_set_explicit(volatile atomic_flag*, memory_order) noexcept;
  bool atomic_flag_test_and_set_explicit(atomic_flag*, memory_order) noexcept;
  void atomic_flag_clear(volatile atomic_flag*) noexcept;
  void atomic_flag_clear(atomic_flag*) noexcept;
  void atomic_flag_clear_explicit(volatile atomic_flag*, memory_order) noexcept;
  void atomic_flag_clear_explicit(atomic_flag*, memory_order) noexcept;

  void atomic_flag_wait(const volatile atomic_flag*, bool) noexcept;
  void atomic_flag_wait(const atomic_flag*, bool) noexcept;
  void atomic_flag_wait_explicit(const volatile atomic_flag*,
                                 bool, memory_order) noexcept;
  void atomic_flag_wait_explicit(const atomic_flag*,
                                 bool, memory_order) noexcept;
  void atomic_flag_notify_one(volatile atomic_flag*) noexcept;
  void atomic_flag_notify_one(atomic_flag*) noexcept;
  void atomic_flag_notify_all(volatile atomic_flag*) noexcept;
  void atomic_flag_notify_all(atomic_flag*) noexcept;

  // \ref{atomics.fences}, fences
  extern "C" void atomic_thread_fence(memory_order) noexcept;
  extern "C" void atomic_signal_fence(memory_order) noexcept;
}
\end{codeblock}

\rSec1[atomics.alias]{Type aliases}
\indexlibraryglobal{atomic_bool}%
\indexlibraryglobal{atomic_char}%
\indexlibraryglobal{atomic_schar}%
\indexlibraryglobal{atomic_uchar}%
\indexlibraryglobal{atomic_short}%
\indexlibraryglobal{atomic_ushort}%
\indexlibraryglobal{atomic_int}%
\indexlibraryglobal{atomic_uint}%
\indexlibraryglobal{atomic_long}%
\indexlibraryglobal{atomic_ulong}%
\indexlibraryglobal{atomic_llong}%
\indexlibraryglobal{atomic_ullong}%
\indexlibraryglobal{atomic_char8_t}%
\indexlibraryglobal{atomic_char16_t}%
\indexlibraryglobal{atomic_char32_t}%
\indexlibraryglobal{atomic_wchar_t}%
\indexlibraryglobal{atomic_int8_t}%
\indexlibraryglobal{atomic_uint8_t}%
\indexlibraryglobal{atomic_int16_t}%
\indexlibraryglobal{atomic_uint16_t}%
\indexlibraryglobal{atomic_int32_t}%
\indexlibraryglobal{atomic_uint32_t}%
\indexlibraryglobal{atomic_int64_t}%
\indexlibraryglobal{atomic_uint64_t}%
\indexlibraryglobal{atomic_int_least8_t}%
\indexlibraryglobal{atomic_uint_least8_t}%
\indexlibraryglobal{atomic_int_least16_t}%
\indexlibraryglobal{atomic_uint_least16_t}%
\indexlibraryglobal{atomic_int_least32_t}%
\indexlibraryglobal{atomic_uint_least32_t}%
\indexlibraryglobal{atomic_int_least64_t}%
\indexlibraryglobal{atomic_uint_least64_t}%
\indexlibraryglobal{atomic_int_fast8_t}%
\indexlibraryglobal{atomic_uint_fast8_t}%
\indexlibraryglobal{atomic_int_fast16_t}%
\indexlibraryglobal{atomic_uint_fast16_t}%
\indexlibraryglobal{atomic_int_fast32_t}%
\indexlibraryglobal{atomic_uint_fast32_t}%
\indexlibraryglobal{atomic_int_fast64_t}%
\indexlibraryglobal{atomic_uint_fast64_t}%
\indexlibraryglobal{atomic_intptr_t}%
\indexlibraryglobal{atomic_uintptr_t}%
\indexlibraryglobal{atomic_size_t}%
\indexlibraryglobal{atomic_ptrdiff_t}%
\indexlibraryglobal{atomic_intmax_t}%
\indexlibraryglobal{atomic_uintmax_t}%
\pnum
The type aliases \tcode{atomic_int$N$_t}, \tcode{atomic_uint$N$_t},
\tcode{atomic_intptr_t}, and \tcode{atomic_uintptr_t}
are defined if and only if
\tcode{int$N$_t}, \tcode{uint$N$_t},
\tcode{intptr_t}, and \tcode{uintptr_t}
are defined, respectively.

\pnum
\indexlibraryglobal{atomic_signed_lock_free}%
\indexlibraryglobal{atomic_unsigned_lock_free}%
The type aliases
\tcode{atomic_signed_lock_free} and \tcode{atomic_unsigned_lock_free}
name specializations of \tcode{atomic}
whose template arguments are integral types, respectively signed and unsigned,
and whose \tcode{is_always_lock_free} property is \tcode{true}.
\begin{note}
\indextext{implementation!freestanding}%
These aliases are optional in freestanding implementations\iref{compliance}.
\end{note}
Implementations should choose for these aliases
the integral specializations of \tcode{atomic}
for which the atomic waiting and notifying operations\iref{atomics.wait}
are most efficient.

\rSec1[atomics.order]{Order and consistency}
\indexlibraryglobal{memory_order}%
\indexlibrarymember{relaxed}{memory_order}%
\indexlibrarymember{consume}{memory_order}%
\indexlibrarymember{acquire}{memory_order}%
\indexlibrarymember{release}{memory_order}%
\indexlibrarymember{acq_rel}{memory_order}%
\indexlibrarymember{seq_cst}{memory_order}%
\indexlibraryglobal{memory_order_relaxed}%
\indexlibraryglobal{memory_order_consume}%
\indexlibraryglobal{memory_order_acquire}%
\indexlibraryglobal{memory_order_release}%
\indexlibraryglobal{memory_order_acq_rel}%
\indexlibraryglobal{memory_order_seq_cst}%

\begin{codeblock}
namespace std {
  enum class memory_order : @\unspec@ {
    relaxed, consume, acquire, release, acq_rel, seq_cst
  };
  inline constexpr memory_order memory_order_relaxed = memory_order::relaxed;
  inline constexpr memory_order memory_order_consume = memory_order::consume;
  inline constexpr memory_order memory_order_acquire = memory_order::acquire;
  inline constexpr memory_order memory_order_release = memory_order::release;
  inline constexpr memory_order memory_order_acq_rel = memory_order::acq_rel;
  inline constexpr memory_order memory_order_seq_cst = memory_order::seq_cst;
}
\end{codeblock}

\pnum
The enumeration \tcode{memory_order} specifies the detailed regular
(non-atomic) memory synchronization order as defined in
\ref{intro.multithread} and may provide for operation ordering. Its
enumerated values and their meanings are as follows:

\begin{itemize}
\item \tcode{memory_order::relaxed}: no operation orders memory.

\item \tcode{memory_order::release}, \tcode{memory_order::acq_rel}, and
\tcode{memory_order::seq_cst}: a store operation performs a release operation on the
affected memory location.

\item \tcode{memory_order::consume}: a load operation performs a consume operation on the
affected memory location.
\begin{note}
Prefer \tcode{memory_order::acquire}, which provides stronger guarantees
than \tcode{memory_order::consume}. Implementations have found it infeasible
to provide performance better than that of \tcode{memory_order::acquire}.
Specification revisions are under consideration.
\end{note}

\item \tcode{memory_order::acquire}, \tcode{memory_order::acq_rel}, and
\tcode{memory_order::seq_cst}: a load operation performs an acquire operation on the
affected memory location.
\end{itemize}

\begin{note}
Atomic operations specifying \tcode{memory_order::relaxed} are relaxed
with respect to memory ordering. Implementations must still guarantee that any
given atomic access to a particular atomic object be indivisible with respect
to all other atomic accesses to that object.
\end{note}

\pnum
An atomic operation $A$ that performs a release operation on an atomic
object $M$ synchronizes with an atomic operation $B$ that performs
an acquire operation on $M$ and takes its value from any side effect in the
release sequence headed by $A$.

\pnum
An atomic operation $A$ on some atomic object $M$ is
\defn{coherence-ordered before}
another atomic operation $B$ on $M$ if
\begin{itemize}
\item $A$ is a modification, and
$B$ reads the value stored by $A$, or
\item $A$ precedes $B$
in the modification order of $M$, or
\item $A$ and $B$ are not
the same atomic read-modify-write operation, and
there exists an atomic modification $X$ of $M$
such that $A$ reads the value stored by $X$ and
$X$ precedes $B$
in the modification order of $M$, or
\item there exists an atomic modification $X$ of $M$
such that $A$ is coherence-ordered before $X$ and
$X$ is coherence-ordered before $B$.
\end{itemize}

\pnum
There is a single total order $S$
on all \tcode{memory_order::seq_cst} operations, including fences,
that satisfies the following constraints.
First, if $A$ and $B$ are
\tcode{memory_order::seq_cst} operations and
$A$ strongly happens before $B$,
then $A$ precedes $B$ in $S$.
Second, for every pair of atomic operations $A$ and
$B$ on an object $M$,
where $A$ is coherence-ordered before $B$,
the following four conditions are required to be satisfied by $S$:
\begin{itemize}
\item if $A$ and $B$ are both
\tcode{memory_order::seq_cst} operations,
then $A$ precedes $B$ in $S$; and
\item if $A$ is a \tcode{memory_order::seq_cst} operation and
$B$ happens before
a \tcode{memory_order::seq_cst} fence $Y$,
then $A$ precedes $Y$ in $S$; and
\item if a \tcode{memory_order::seq_cst} fence $X$
happens before $A$ and
$B$ is a \tcode{memory_order::seq_cst} operation,
then $X$ precedes $B$ in $S$; and
\item if a \tcode{memory_order::seq_cst} fence $X$
happens before $A$ and
$B$ happens before
a \tcode{memory_order::seq_cst} fence $Y$,
then $X$ precedes $Y$ in $S$.
\end{itemize}

\pnum
\begin{note}
This definition ensures that $S$ is consistent with
the modification order of any atomic object $M$.
It also ensures that
a \tcode{memory_order::seq_cst} load $A$ of $M$
gets its value either from the last modification of $M$
that precedes $A$ in $S$ or
from some non-\tcode{memory_order::seq_cst} modification of $M$
that does not happen before any modification of $M$
that precedes $A$ in $S$.
\end{note}

\pnum
\begin{note}
We do not require that $S$ be consistent with
``happens before''\iref{intro.races}.
This allows more efficient implementation
of \tcode{memory_order::acquire} and \tcode{memory_order::release}
on some machine architectures.
It can produce surprising results
when these are mixed with \tcode{memory_order::seq_cst} accesses.
\end{note}

\pnum
\begin{note}
\tcode{memory_order::seq_cst} ensures sequential consistency only
for a program that is free of data races and
uses exclusively \tcode{memory_order::seq_cst} atomic operations.
Any use of weaker ordering will invalidate this guarantee
unless extreme care is used.
In many cases, \tcode{memory_order::seq_cst} atomic operations are reorderable
with respect to other atomic operations performed by the same thread.
\end{note}

\pnum
Implementations should ensure that no ``out-of-thin-air'' values are computed that
circularly depend on their own computation.

\begin{note}
For example, with \tcode{x} and \tcode{y} initially zero,
\begin{codeblock}
// Thread 1:
r1 = y.load(memory_order::relaxed);
x.store(r1, memory_order::relaxed);
\end{codeblock}

\begin{codeblock}
// Thread 2:
r2 = x.load(memory_order::relaxed);
y.store(r2, memory_order::relaxed);
\end{codeblock}
this recommendation discourages producing \tcode{r1 == r2 == 42}, since the store of 42 to \tcode{y} is only
possible if the store to \tcode{x} stores \tcode{42}, which circularly depends on the
store to \tcode{y} storing \tcode{42}. Note that without this restriction, such an
execution is possible.
\end{note}

\pnum
\begin{note}
The recommendation similarly disallows \tcode{r1 == r2 == 42} in the
following example, with \tcode{x} and \tcode{y} again initially zero:

\begin{codeblock}
// Thread 1:
r1 = x.load(memory_order::relaxed);
if (r1 == 42) y.store(42, memory_order::relaxed);
\end{codeblock}

\begin{codeblock}
// Thread 2:
r2 = y.load(memory_order::relaxed);
if (r2 == 42) x.store(42, memory_order::relaxed);
\end{codeblock}
\end{note}

\pnum
Atomic read-modify-write operations shall always read the last value
(in the modification order) written before the write associated with
the read-modify-write operation.

\pnum
Implementations should make atomic stores visible to atomic loads within a reasonable
amount of time.

\indexlibraryglobal{kill_dependency}%
\begin{itemdecl}
template<class T>
  T kill_dependency(T y) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
The argument does not carry a dependency to the return
value\iref{intro.multithread}.

\pnum
\returns
\tcode{y}.
\end{itemdescr}


\rSec1[atomics.lockfree]{Lock-free property}

\indexlibraryglobal{ATOMIC_BOOL_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_CHAR_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_CHAR8_T_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_CHAR16_T_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_CHAR32_T_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_WCHAR_T_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_SHORT_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_INT_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_LONG_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_LLONG_LOCK_FREE}%
\indexlibraryglobal{ATOMIC_POINTER_LOCK_FREE}%
\indeximpldef{values of various \tcode{ATOMIC_..._LOCK_FREE} macros}
\begin{codeblock}
#define ATOMIC_BOOL_LOCK_FREE @\unspec@
#define ATOMIC_CHAR_LOCK_FREE @\unspec@
#define ATOMIC_CHAR8_T_LOCK_FREE @\unspec@
#define ATOMIC_CHAR16_T_LOCK_FREE @\unspec@
#define ATOMIC_CHAR32_T_LOCK_FREE @\unspec@
#define ATOMIC_WCHAR_T_LOCK_FREE @\unspec@
#define ATOMIC_SHORT_LOCK_FREE @\unspec@
#define ATOMIC_INT_LOCK_FREE @\unspec@
#define ATOMIC_LONG_LOCK_FREE @\unspec@
#define ATOMIC_LLONG_LOCK_FREE @\unspec@
#define ATOMIC_POINTER_LOCK_FREE @\unspec@
\end{codeblock}

\pnum
The \tcode{ATOMIC_..._LOCK_FREE} macros indicate the lock-free property of the
corresponding atomic types, with the signed and unsigned variants grouped
together. The properties also apply to the corresponding (partial) specializations of the
\tcode{atomic} template. A value of 0 indicates that the types are never
lock-free. A value of 1 indicates that the types are sometimes lock-free. A
value of 2 indicates that the types are always lock-free.

\pnum
At least one signed integral specialization of the \tcode{atomic} template,
along with the specialization
for the corresponding unsigned type\iref{basic.fundamental},
is always lock-free.
\begin{note}
\indextext{implementation!freestanding}%
This requirement is optional in freestanding implementations\iref{compliance}.
\end{note}

\pnum
The functions \tcode{atomic<T>::is_lock_free} and
\tcode{atomic_is_lock_free}\iref{atomics.types.operations}
indicate whether the object is lock-free. In any given program execution, the
result of the lock-free query
is the same for all atomic objects of the same type.

\pnum
Atomic operations that are not lock-free are considered to potentially
block\iref{intro.progress}.

\pnum
\recommended
Operations that are lock-free should also be address-free.
\begin{footnote}
That is,
atomic operations on the same memory location via two different addresses will
communicate atomically.
\end{footnote}
The implementation of these operations should not depend on any per-process state.
\begin{note}
This restriction enables communication by memory that is
mapped into a process more than once and by memory that is shared between two
processes.
\end{note}

\rSec1[atomics.wait]{Waiting and notifying}

\pnum
\defnx{Atomic waiting operations}{atomic!waiting operation}
and \defnx{atomic notifying operations}{atomic!notifying operation}
provide a mechanism to wait for the value of an atomic object to change
more efficiently than can be achieved with polling.
An atomic waiting operation may block until it is unblocked
by an atomic notifying operation, according to each function's effects.
\begin{note}
Programs are not guaranteed to observe transient atomic values,
an issue known as the A-B-A problem,
resulting in continued blocking if a condition is only temporarily met.
\end{note}

\pnum
\begin{note}
The following functions are atomic waiting operations:
\begin{itemize}
\item \tcode{atomic<T>::wait},
\item \tcode{atomic_flag::wait},
\item \tcode{atomic_wait} and \tcode{atomic_wait_explicit},
\item \tcode{atomic_flag_wait} and \tcode{atomic_flag_wait_explicit}, and
\item \tcode{atomic_ref<T>::wait}.
\end{itemize}
\end{note}

\pnum
\begin{note}
The following functions are atomic notifying operations:
\begin{itemize}
\item \tcode{atomic<T>::notify_one} and \tcode{atomic<T>::notify_all},
\item \tcode{atomic_flag::notify_one} and \tcode{atomic_flag::notify_all},
\item \tcode{atomic_notify_one} and \tcode{atomic_notify_all},
\item \tcode{atomic_flag_notify_one} and \tcode{atomic_flag_notify_all}, and
\item \tcode{atomic_ref<T>::notify_one} and \tcode{atomic_ref<T>::notify_all}.
\end{itemize}
\end{note}

\indextext{atomic!waiting operation!eligible to be unblocked}%
\pnum
A call to an atomic waiting operation on an atomic object \tcode{M}
is \defn{eligible to be unblocked}
by a call to an atomic notifying operation on \tcode{M}
if there exist side effects \tcode{X} and \tcode{Y} on \tcode{M} such that:
\begin{itemize}
\item the atomic waiting operation has blocked after observing the result of \tcode{X},
\item \tcode{X} precedes \tcode{Y} in the modification order of \tcode{M}, and
\item \tcode{Y} happens before the call to the atomic notifying operation.
\end{itemize}

\rSec1[atomics.ref.generic]{Class template \tcode{atomic_ref}}

\rSec2[atomics.ref.generic.general]{General}

\indexlibraryglobal{atomic_ref}%
\indexlibrarymember{value_type}{atomic_ref}%
\begin{codeblock}
namespace std {
  template<class T> struct atomic_ref {
  private:
    T* ptr;             // \expos
  public:
    using value_type = T;
    static constexpr size_t required_alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    bool is_lock_free() const noexcept;

    explicit atomic_ref(T&);
    atomic_ref(const atomic_ref&) noexcept;
    atomic_ref& operator=(const atomic_ref&) = delete;

    void store(T, memory_order = memory_order::seq_cst) const noexcept;
    T operator=(T) const noexcept;
    T load(memory_order = memory_order::seq_cst) const noexcept;
    operator T() const noexcept;

    T exchange(T, memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_weak(T&, T,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(T&, T,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(T&, T,
                               memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_strong(T&, T,
                                 memory_order = memory_order::seq_cst) const noexcept;

    void wait(T, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() const noexcept;
    void notify_all() const noexcept;
  };
}
\end{codeblock}

\pnum
An \tcode{atomic_ref} object applies atomic operations\iref{atomics.general} to
the object referenced by \tcode{*ptr} such that,
for the lifetime\iref{basic.life} of the \tcode{atomic_ref} object,
the object referenced by \tcode{*ptr} is an atomic object\iref{intro.races}.

\pnum
The program is ill-formed if \tcode{is_trivially_copyable_v<T>} is \tcode{false}.

\pnum
The lifetime\iref{basic.life} of an object referenced by \tcode{*ptr}
shall exceed the lifetime of all \tcode{atomic_ref}s that reference the object.
While any \tcode{atomic_ref} instances exist
that reference the \tcode{*ptr} object,
all accesses to that object shall exclusively occur
through those \tcode{atomic_ref} instances.
No subobject of the object referenced by \tcode{atomic_ref}
shall be concurrently referenced by any other \tcode{atomic_ref} object.

\pnum
Atomic operations applied to an object
through a referencing \tcode{atomic_ref} are atomic with respect to
atomic operations applied through any other \tcode{atomic_ref}
referencing the same object.
\begin{note}
Atomic operations or the \tcode{atomic_ref} constructor can acquire
a shared resource, such as a lock associated with the referenced object,
to enable atomic operations to be applied to the referenced object.
\end{note}

\rSec2[atomics.ref.ops]{Operations}

\indexlibrarymember{required_alignment}{atomic_ref}%
\indexlibrarymember{required_alignment}{atomic_ref<T*>}%
\indexlibrarymember{required_alignment}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{required_alignment}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
static constexpr size_t required_alignment;
\end{itemdecl}

\begin{itemdescr}
\pnum
The alignment required for an object to be referenced by an atomic reference,
which is at least \tcode{alignof(T)}.

\pnum
\begin{note}
Hardware could require an object
referenced by an \tcode{atomic_ref}
to have stricter alignment\iref{basic.align}
than other objects of type \tcode{T}.
Further, whether operations on an \tcode{atomic_ref}
are lock-free could depend on the alignment of the referenced object.
For example, lock-free operations on \tcode{std::complex<double>}
could be supported only if aligned to \tcode{2*alignof(double)}.
\end{note}
\end{itemdescr}

\indexlibrarymember{is_always_lock_free}{atomic_ref}%
\indexlibrarymember{is_always_lock_free}{atomic_ref<T*>}%
\indexlibrarymember{is_always_lock_free}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{is_always_lock_free}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
static constexpr bool is_always_lock_free;
\end{itemdecl}

\begin{itemdescr}
\pnum
The static data member \tcode{is_always_lock_free} is \tcode{true}
if the \tcode{atomic_ref} type's operations are always lock-free,
and \tcode{false} otherwise.
\end{itemdescr}

\indexlibrarymember{is_lock_free}{atomic_ref}%
\indexlibrarymember{is_lock_free}{atomic_ref<T*>}%
\indexlibrarymember{is_lock_free}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{is_lock_free}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
bool is_lock_free() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\returns
\tcode{true} if operations on all objects of the type \tcode{atomic_ref<T>}
are lock-free,
\tcode{false} otherwise.
\end{itemdescr}

\indexlibraryctor{atomic_ref}%
\indexlibraryctor{atomic_ref<T*>}%
\indexlibrary{\idxcode{atomic_ref<\placeholder{integral}>}!constructor}%
\indexlibrary{\idxcode{atomic_ref<\placeholder{floating-point}>}!constructor}%
\begin{itemdecl}
atomic_ref(T& obj);
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
The referenced object is aligned to \tcode{required_alignment}.

\pnum
\ensures
\tcode{*this} references \tcode{obj}.

\pnum
\throws
Nothing.
\end{itemdescr}

\indexlibraryctor{atomic_ref}%
\indexlibraryctor{atomic_ref<T*>}%
\indexlibrary{\idxcode{atomic_ref<\placeholder{integral}>}!constructor}%
\indexlibrary{\idxcode{atomic_ref<\placeholder{floating-point}>}!constructor}%
\begin{itemdecl}
atomic_ref(const atomic_ref& ref) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\ensures
\tcode{*this} references the object referenced by \tcode{ref}.
\end{itemdescr}

\indexlibrarymember{store}{atomic_ref}%
\indexlibrarymember{store}{atomic_ref<T*>}%
\indexlibrarymember{store}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{store}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
void store(T desired, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
The \tcode{order} argument is neither
\tcode{memory_order::consume},
\tcode{memory_order::acquire}, nor
\tcode{memory_order::acq_rel}.

\pnum
\effects
Atomically replaces the value referenced by \tcode{*ptr}
with the value of \tcode{desired}.
Memory is affected according to the value of \tcode{order}.
\end{itemdescr}

\indexlibrarymember{operator=}{atomic_ref}%
\indexlibrarymember{operator=}{atomic_ref<T*>}%
\indexlibrarymember{operator=}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{operator=}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
T operator=(T desired) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\begin{codeblock}
store(desired);
return desired;
\end{codeblock}
\end{itemdescr}

\indexlibrarymember{load}{atomic_ref}%
\indexlibrarymember{load}{atomic_ref<T*>}%
\indexlibrarymember{load}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{load}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
T load(memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
The \tcode{order} argument is neither
\tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Memory is affected according to the value of \tcode{order}.

\pnum
\returns
Atomically returns the value referenced by \tcode{*ptr}.
\end{itemdescr}

\indexlibrarymember{operator \placeholder{type}}{atomic_ref}%
\indexlibrarymember{operator T*}{atomic_ref<T*>}%
\indexlibrarymember{operator \placeholder{integral}}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{operator \placeholder{floating-point}}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
operator T() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return load();}
\end{itemdescr}

\indexlibrarymember{exchange}{atomic_ref}%
\indexlibrarymember{exchange}{atomic_ref<T*>}%
\indexlibrarymember{exchange}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{exchange}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
T exchange(T desired, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Atomically replaces the value referenced by \tcode{*ptr}
with \tcode{desired}.
Memory is affected according to the value of \tcode{order}.
This operation is an atomic read-modify-write operation\iref{intro.multithread}.

\pnum
\returns
Atomically returns the value referenced by \tcode{*ptr}
immediately before the effects.
\end{itemdescr}

\indexlibrarymember{compare_exchange_weak}{atomic_ref}%
\indexlibrarymember{compare_exchange_weak}{atomic_ref<T*>}%
\indexlibrarymember{compare_exchange_weak}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{compare_exchange_weak}{atomic_ref<\placeholder{floating-point}>}%
\indexlibrarymember{compare_exchange_strong}{atomic_ref}%
\indexlibrarymember{compare_exchange_strong}{atomic_ref<T*>}%
\indexlibrarymember{compare_exchange_strong}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{compare_exchange_strong}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
bool compare_exchange_weak(T& expected, T desired,
                           memory_order success, memory_order failure) const noexcept;

bool compare_exchange_strong(T& expected, T desired,
                             memory_order success, memory_order failure) const noexcept;

bool compare_exchange_weak(T& expected, T desired,
                           memory_order order = memory_order::seq_cst) const noexcept;

bool compare_exchange_strong(T& expected, T desired,
                             memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
The \tcode{failure} argument is neither
\tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Retrieves the value in \tcode{expected}.
It then atomically compares the value representation of
the value referenced by \tcode{*ptr} for equality
with that previously retrieved from \tcode{expected},
and if \tcode{true}, replaces the value referenced by \tcode{*ptr}
with that in \tcode{desired}.
If and only if the comparison is \tcode{true},
memory is affected according to the value of \tcode{success}, and
if the comparison is \tcode{false},
memory is affected according to the value of \tcode{failure}.
When only one \tcode{memory_order} argument is supplied,
the value of \tcode{success} is \tcode{order}, and
the value of \tcode{failure} is \tcode{order}
except that a value of \tcode{memory_order::acq_rel} shall be replaced by
the value \tcode{memory_order::acquire} and
a value of \tcode{memory_order::release} shall be replaced by
the value \tcode{memory_order::relaxed}.
If and only if the comparison is \tcode{false} then,
after the atomic operation,
the value in \tcode{expected} is replaced by
the value read from the value referenced by \tcode{*ptr}
during the atomic comparison.
If the operation returns \tcode{true},
these operations are atomic read-modify-write operations\iref{intro.races}
on the value referenced by \tcode{*ptr}.
Otherwise, these operations are atomic load operations on that memory.

\pnum
\returns
The result of the comparison.

\pnum
\remarks
A weak compare-and-exchange operation may fail spuriously.
That is, even when the contents of memory referred to
by \tcode{expected} and \tcode{ptr} are equal,
it may return \tcode{false} and
store back to \tcode{expected} the same memory contents
that were originally there.
\begin{note}
This spurious failure enables implementation of compare-and-exchange
on a broader class of machines, e.g., load-locked store-conditional machines.
A consequence of spurious failure is
that nearly all uses of weak compare-and-exchange will be in a loop.
When a compare-and-exchange is in a loop,
the weak version will yield better performance on some platforms.
When a weak compare-and-exchange would require a loop and
a strong one would not, the strong one is preferable.
\end{note}
\end{itemdescr}

\indexlibrarymember{wait}{atomic_ref<T>}%
\begin{itemdecl}
void wait(T old, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is
neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Repeatedly performs the following steps, in order:
\begin{itemize}
\item
  Evaluates \tcode{load(order)} and
  compares its value representation for equality against that of \tcode{old}.
\item
  If they compare unequal, returns.
\item
  Blocks until it
  is unblocked by an atomic notifying operation or is unblocked spuriously.
\end{itemize}

\pnum
\remarks
This function is an atomic waiting operation\iref{atomics.wait}
on atomic object \tcode{*ptr}.
\end{itemdescr}

\indexlibrarymember{notify_one}{atomic_ref<T>}%
\begin{itemdecl}
void notify_one() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of at least one atomic waiting operation on \tcode{*ptr}
that is eligible to be unblocked\iref{atomics.wait} by this call,
if any such atomic waiting operations exist.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}
on atomic object \tcode{*ptr}.
\end{itemdescr}

\indexlibrarymember{notify_all}{atomic_ref<T>}%
\begin{itemdecl}
void notify_all() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of all atomic waiting operations on \tcode{*ptr}
that are eligible to be unblocked\iref{atomics.wait} by this call.

\pnum
\remarks
 This function is an atomic notifying operation\iref{atomics.wait}
 on atomic object \tcode{*ptr}.
\end{itemdescr}

\rSec2[atomics.ref.int]{Specializations for integral types}

\pnum
\indexlibrary{\idxcode{atomic_ref<\placeholder{integral}>}}%
There are specializations of the \tcode{atomic_ref} class template
for the integral types
\tcode{char},
\tcode{signed char},
\tcode{unsigned char},
\tcode{short},
\tcode{unsigned short},
\tcode{int},
\tcode{unsigned int},
\tcode{long},
\tcode{unsigned long},
\tcode{long long},
\tcode{unsigned long long},
\keyword{char8_t},
\keyword{char16_t},
\keyword{char32_t},
\keyword{wchar_t},
and any other types needed by the typedefs in the header \libheaderref{cstdint}.
For each such type \tcode{\placeholder{integral}},
the specialization \tcode{atomic_ref<\placeholder{integral}>} provides
additional atomic operations appropriate to integral types.
\begin{note}
The specialization \tcode{atomic_ref<bool>}
uses the primary template\iref{atomics.ref.generic}.
\end{note}

\begin{codeblock}
namespace std {
  template<> struct atomic_ref<@\placeholder{integral}@> {
  private:
    @\placeholder{integral}@* ptr;        // \expos
  public:
    using value_type = @\placeholder{integral}@;
    using difference_type = value_type;
    static constexpr size_t required_alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    bool is_lock_free() const noexcept;

    explicit atomic_ref(@\placeholder{integral}@&);
    atomic_ref(const atomic_ref&) noexcept;
    atomic_ref& operator=(const atomic_ref&) = delete;

    void store(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) const noexcept;
    @\placeholdernc{integral}@ operator=(@\placeholder{integral}@) const noexcept;
    @\placeholdernc{integral}@ load(memory_order = memory_order::seq_cst) const noexcept;
    operator @\placeholdernc{integral}@() const noexcept;

    @\placeholdernc{integral}@ exchange(@\placeholdernc{integral}@,
                      memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholder{integral}@,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholder{integral}@,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholder{integral}@,
                               memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholder{integral}@,
                                 memory_order = memory_order::seq_cst) const noexcept;

    @\placeholdernc{integral}@ fetch_add(@\placeholdernc{integral}@,
                       memory_order = memory_order::seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_sub(@\placeholdernc{integral}@,
                       memory_order = memory_order::seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_and(@\placeholdernc{integral}@,
                       memory_order = memory_order::seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_or(@\placeholdernc{integral}@,
                      memory_order = memory_order::seq_cst) const noexcept;
    @\placeholdernc{integral}@ fetch_xor(@\placeholdernc{integral}@,
                       memory_order = memory_order::seq_cst) const noexcept;

    @\placeholdernc{integral}@ operator++(int) const noexcept;
    @\placeholdernc{integral}@ operator--(int) const noexcept;
    @\placeholdernc{integral}@ operator++() const noexcept;
    @\placeholdernc{integral}@ operator--() const noexcept;
    @\placeholdernc{integral}@ operator+=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator-=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator&=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator|=(@\placeholdernc{integral}@) const noexcept;
    @\placeholdernc{integral}@ operator^=(@\placeholdernc{integral}@) const noexcept;

    void wait(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() const noexcept;
    void notify_all() const noexcept;
  };
}
\end{codeblock}

\pnum
Descriptions are provided below only for members
that differ from the primary template.

\pnum
The following operations perform arithmetic computations.
The key, operator, and computation correspondence is identified
in \tref{atomic.types.int.comp}.

\indexlibrarymember{fetch_add}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{fetch_and}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{fetch_or}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{fetch_sub}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{fetch_xor}{atomic_ref<\placeholder{integral}>}%
\begin{itemdecl}
@\placeholdernc{integral}@ fetch_@\placeholdernc{key}@(@\placeholdernc{integral}@ operand, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Atomically replaces the value referenced by \tcode{*ptr} with
the result of the computation applied to the value referenced by \tcode{*ptr}
and the given operand.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.races}.

\pnum
\returns
Atomically, the value referenced by \tcode{*ptr}
immediately before the effects.

\pnum
\indextext{signed integer representation!two's complement}%
\remarks
For signed integer types,
the result is as if the object value and parameters
were converted to their corresponding unsigned types,
the computation performed on those types, and
the result converted back to the signed type.
\begin{note}
There are no undefined results arising from the computation.
\end{note}
\end{itemdescr}

\indexlibrarymember{operator+=}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{operator-=}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{operator\&=}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{operator"|=}{atomic_ref<\placeholder{integral}>}%
\indexlibrarymember{operator\caret=}{atomic_ref<\placeholder{integral}>}%
\begin{itemdecl}
@\placeholdernc{integral}@ operator @\placeholder{op}@=(@\placeholdernc{integral}@ operand) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\tcode{return fetch_\placeholdernc{key}(operand) \placeholder{op} operand;}
\end{itemdescr}

\rSec2[atomics.ref.float]{Specializations for floating-point types}

\pnum
\indexlibrary{\idxcode{atomic_ref<\placeholder{floating-point}>}}%
There are specializations of the \tcode{atomic_ref} class template
for the floating-point types
\tcode{float},
\tcode{double}, and
\tcode{long double}.
For each such type \tcode{\placeholder{floating-point}},
the specialization \tcode{atomic_ref<\placeholder{floating-\-point}>} provides
additional atomic operations appropriate to floating-point types.

\begin{codeblock}
namespace std {
  template<> struct atomic_ref<@\placeholder{floating-point}@> {
  private:
    @\placeholder{floating-point}@* ptr;  // \expos
  public:
    using value_type = @\placeholder{floating-point}@;
    using difference_type = value_type;
    static constexpr size_t required_alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    bool is_lock_free() const noexcept;

    explicit atomic_ref(@\placeholder{floating-point}@&);
    atomic_ref(const atomic_ref&) noexcept;
    atomic_ref& operator=(const atomic_ref&) = delete;

    void store(@\placeholdernc{floating-point}@, memory_order = memory_order::seq_cst) const noexcept;
    @\placeholder{floating-point}@ operator=(@\placeholder{floating-point}@) const noexcept;
    @\placeholder{floating-point}@ load(memory_order = memory_order::seq_cst) const noexcept;
    operator @\placeholdernc{floating-point}@() const noexcept;

    @\placeholder{floating-point}@ exchange(@\placeholdernc{floating-point}@,
                            memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                               memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                                 memory_order = memory_order::seq_cst) const noexcept;

    @\placeholder{floating-point}@ fetch_add(@\placeholdernc{floating-point}@,
                             memory_order = memory_order::seq_cst) const noexcept;
    @\placeholder{floating-point}@ fetch_sub(@\placeholdernc{floating-point}@,
                             memory_order = memory_order::seq_cst) const noexcept;

    @\placeholder{floating-point}@ operator+=(@\placeholder{floating-point}@) const noexcept;
    @\placeholder{floating-point}@ operator-=(@\placeholder{floating-point}@) const noexcept;

    void wait(@\placeholdernc{floating-point}@, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() const noexcept;
    void notify_all() const noexcept;
  };
}
\end{codeblock}

\pnum
Descriptions are provided below only for members
that differ from the primary template.

\pnum
The following operations perform arithmetic computations.
The key, operator, and computation correspondence are identified
in \tref{atomic.types.int.comp}.

\indexlibrarymember{fetch_add}{atomic_ref<\placeholder{floating-point}>}%
\indexlibrarymember{fetch_sub}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
@\placeholder{floating-point}@ fetch_@\placeholdernc{key}@(@\placeholder{floating-point}@ operand,
                          memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Atomically replaces the value referenced by \tcode{*ptr} with
the result of the computation applied to the value referenced by \tcode{*ptr}
and the given operand.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.races}.

\pnum
\returns
Atomically, the value referenced by \tcode{*ptr}
immediately before the effects.

\pnum
\remarks
If the result is not a representable value for its type\iref{expr.pre},
the result is unspecified,
but the operations otherwise have no undefined behavior.
Atomic arithmetic operations on \tcode{\placeholder{floating-point}} should conform to
the \tcode{std::numeric_limits<\placeholder{floating-point}>} traits
associated with the floating-point type\iref{limits.syn}.
The floating-point environment\iref{cfenv}
for atomic arithmetic operations on \tcode{\placeholder{floating-point}}
may be different than the calling thread's floating-point environment.
\end{itemdescr}

\indexlibrarymember{operator+=}{atomic_ref<\placeholder{floating-point}>}%
\indexlibrarymember{operator-=}{atomic_ref<\placeholder{floating-point}>}%
\begin{itemdecl}
@\placeholder{floating-point}@ operator @\placeholder{op}@=(@\placeholder{floating-point}@ operand) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\tcode{return fetch_\placeholder{key}(operand) \placeholdernc{op} operand;}
\end{itemdescr}

\rSec2[atomics.ref.pointer]{Partial specialization for pointers}
\indexlibraryglobal{atomic_ref<T*>}%

\begin{codeblock}
namespace std {
  template<class T> struct atomic_ref<T*> {
  private:
    T** ptr;        // \expos
  public:
    using value_type = T*;
    using difference_type = ptrdiff_t;
    static constexpr size_t required_alignment = @\impdefx{required alignment for \tcode{atomic_ref} type's operations}@;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic_ref} type's operations are always lock free}@;
    bool is_lock_free() const noexcept;

    explicit atomic_ref(T*&);
    atomic_ref(const atomic_ref&) noexcept;
    atomic_ref& operator=(const atomic_ref&) = delete;

    void store(T*, memory_order = memory_order::seq_cst) const noexcept;
    T* operator=(T*) const noexcept;
    T* load(memory_order = memory_order::seq_cst) const noexcept;
    operator T*() const noexcept;

    T* exchange(T*, memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order, memory_order) const noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order, memory_order) const noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order = memory_order::seq_cst) const noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order = memory_order::seq_cst) const noexcept;

    T* fetch_add(difference_type, memory_order = memory_order::seq_cst) const noexcept;
    T* fetch_sub(difference_type, memory_order = memory_order::seq_cst) const noexcept;

    T* operator++(int) const noexcept;
    T* operator--(int) const noexcept;
    T* operator++() const noexcept;
    T* operator--() const noexcept;
    T* operator+=(difference_type) const noexcept;
    T* operator-=(difference_type) const noexcept;

    void wait(T*, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() const noexcept;
    void notify_all() const noexcept;
  };
}
\end{codeblock}

\pnum
Descriptions are provided below only for members
that differ from the primary template.

\pnum
The following operations perform arithmetic computations.
The key, operator, and computation correspondence is identified
in \tref{atomic.types.pointer.comp}.

\indexlibrarymember{fetch_add}{atomic_ref<T*>}%
\indexlibrarymember{fetch_sub}{atomic_ref<T*>}%
\begin{itemdecl}
T* fetch_@\placeholdernc{key}@(difference_type operand, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates
\tcode{T} is a complete object type.

\pnum
\effects
Atomically replaces the value referenced by \tcode{*ptr} with
the result of the computation applied to the value referenced by \tcode{*ptr}
and the given operand.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.races}.

\pnum
\returns
Atomically, the value referenced by \tcode{*ptr}
immediately before the effects.

\pnum
\remarks
The result may be an undefined address,
but the operations otherwise have no undefined behavior.
\end{itemdescr}

\indexlibrarymember{operator+=}{atomic_ref<T*>}%
\indexlibrarymember{operator-=}{atomic_ref<T*>}%
\begin{itemdecl}
T* operator @\placeholder{op}@=(difference_type operand) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\tcode{return fetch_\placeholder{key}(operand) \placeholdernc{op} operand;}
\end{itemdescr}

\rSec2[atomics.ref.memop]{Member operators
                          common to integers and pointers to objects}

\indexlibrarymember{operator++}{atomic_ref<T*>}%
\indexlibrarymember{operator++}{atomic_ref<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator++(int) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return fetch_add(1);}
\end{itemdescr}

\indexlibrarymember{operator--}{atomic_ref<T*>}%
\indexlibrarymember{operator--}{atomic_ref<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator--(int) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return fetch_sub(1);}
\end{itemdescr}

\indexlibrarymember{operator++}{atomic_ref<T*>}%
\indexlibrarymember{operator++}{atomic_ref<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator++() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return fetch_add(1) + 1;}
\end{itemdescr}

\indexlibrarymember{operator--}{atomic_ref<T*>}%
\indexlibrarymember{operator--}{atomic_ref<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator--() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return fetch_sub(1) - 1;}
\end{itemdescr}

\rSec1[atomics.types.generic]{Class template \tcode{atomic}}

\rSec2[atomics.types.generic.general]{General}

\indexlibraryglobal{atomic}%
\indexlibrarymember{value_type}{atomic}%
\begin{codeblock}
namespace std {
  template<class T> struct atomic {
    using value_type = T;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;

    // \ref{atomics.types.operations}, operations on atomic types
    constexpr atomic() noexcept(is_nothrow_default_constructible_v<T>);
    constexpr atomic(T) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;

    T load(memory_order = memory_order::seq_cst) const volatile noexcept;
    T load(memory_order = memory_order::seq_cst) const noexcept;
    operator T() const volatile noexcept;
    operator T() const noexcept;
    void store(T, memory_order = memory_order::seq_cst) volatile noexcept;
    void store(T, memory_order = memory_order::seq_cst) noexcept;
    T operator=(T) volatile noexcept;
    T operator=(T) noexcept;

    T exchange(T, memory_order = memory_order::seq_cst) volatile noexcept;
    T exchange(T, memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_weak(T&, T, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(T&, T, memory_order, memory_order) noexcept;
    bool compare_exchange_strong(T&, T, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(T&, T, memory_order, memory_order) noexcept;
    bool compare_exchange_weak(T&, T, memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_weak(T&, T, memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_strong(T&, T, memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_strong(T&, T, memory_order = memory_order::seq_cst) noexcept;

    void wait(T, memory_order = memory_order::seq_cst) const volatile noexcept;
    void wait(T, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() volatile noexcept;
    void notify_one() noexcept;
    void notify_all() volatile noexcept;
    void notify_all() noexcept;
  };
}
\end{codeblock}

\indexlibraryglobal{atomic}%
\pnum
The template argument for \tcode{T} shall meet the
\oldconcept{CopyConstructible} and \oldconcept{CopyAssignable} requirements.
The program is ill-formed if any of
\begin{itemize}
\item \tcode{is_trivially_copyable_v<T>},
\item \tcode{is_copy_constructible_v<T>},
\item \tcode{is_move_constructible_v<T>},
\item \tcode{is_copy_assignable_v<T>}, or
\item \tcode{is_move_assignable_v<T>}
\end{itemize}
is \tcode{false}.
\begin{note}
Type arguments that are
not also statically initializable can be difficult to use.
\end{note}

\pnum
The specialization \tcode{atomic<bool>} is a standard-layout struct.

\pnum
\begin{note}
The representation of an atomic specialization
need not have the same size and alignment requirement as
its corresponding argument type.
\end{note}

\rSec2[atomics.types.operations]{Operations on atomic types}

\indexlibraryctor{atomic}%
\indexlibraryctor{atomic<T*>}%
\indexlibrary{\idxcode{atomic<\placeholder{integral}>}!constructor}%
\indexlibrary{\idxcode{atomic<\placeholder{floating-point}>}!constructor}%
\begin{itemdecl}
constexpr atomic() noexcept(is_nothrow_default_constructible_v<T>);
\end{itemdecl}

\begin{itemdescr}
\pnum
\mandates
\tcode{is_default_constructible_v<T>} is \tcode{true}.

\pnum
\effects
Initializes the atomic object with the value of \tcode{T()}.
Initialization is not an atomic operation\iref{intro.multithread}.
\end{itemdescr}

\indexlibraryctor{atomic}%
\indexlibraryctor{atomic<T*>}%
\indexlibrary{\idxcode{atomic<\placeholder{integral}>}!constructor}%
\indexlibrary{\idxcode{atomic<\placeholder{floating-point}>}!constructor}%
\begin{itemdecl}
constexpr atomic(T desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Initializes the object with the value \tcode{desired}.
Initialization is not an atomic operation\iref{intro.multithread}.
\begin{note}
It is possible to have an access to an atomic object \tcode{A}
race with its construction, for example by communicating the address of the
just-constructed object \tcode{A} to another thread via
\tcode{memory_order::relaxed} operations on a suitable atomic pointer
variable, and then immediately accessing \tcode{A} in the receiving thread.
This results in undefined behavior.
\end{note}
\end{itemdescr}

\indexlibrarymember{is_always_lock_free}{atomic}%
\indexlibrarymember{is_always_lock_free}{atomic<T*>}%
\indexlibrarymember{is_always_lock_free}{atomic<\placeholder{integral}>}%
\indexlibrarymember{is_always_lock_free}{atomic<\placeholder{floating-point}>}%
\indexlibrarymember{is_always_lock_free}{atomic<shared_ptr<T>>}%
\indexlibrarymember{is_always_lock_free}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
\end{itemdecl}

\begin{itemdescr}
\pnum
The \keyword{static} data member \tcode{is_always_lock_free} is \tcode{true}
if the atomic type's operations are always lock-free, and \tcode{false} otherwise.
\begin{note}
The value of \tcode{is_always_lock_free} is consistent with the value of
the corresponding \tcode{ATOMIC_..._LOCK_FREE} macro, if defined.
\end{note}
\end{itemdescr}

\indexlibraryglobal{atomic_is_lock_free}%
\indexlibrarymember{is_lock_free}{atomic}%
\indexlibrarymember{is_lock_free}{atomic<T*>}%
\indexlibrarymember{is_lock_free}{atomic<\placeholder{integral}>}%
\indexlibrarymember{is_lock_free}{atomic<\placeholder{floating-point}>}%
\indexlibrarymember{is_lock_free}{atomic<shared_ptr<T>>}%
\indexlibrarymember{is_lock_free}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
bool is_lock_free() const volatile noexcept;
bool is_lock_free() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\returns
\tcode{true} if the object's operations are lock-free, \tcode{false} otherwise.
\begin{note}
The return value of the \tcode{is_lock_free} member function
is consistent with the value of \tcode{is_always_lock_free} for the same type.
\end{note}
\end{itemdescr}

\indexlibraryglobal{atomic_store}%
\indexlibraryglobal{atomic_store_explicit}%
\indexlibrarymember{store}{atomic}%
\indexlibrarymember{store}{atomic<T*>}%
\indexlibrarymember{store}{atomic<\placeholder{integral}>}%
\indexlibrarymember{store}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
void store(T desired, memory_order order = memory_order::seq_cst) volatile noexcept;
void store(T desired, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\expects
The \tcode{order} argument is neither \tcode{memory_order::consume},
\tcode{memory_order::acquire}, nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Atomically replaces the value pointed to by \keyword{this}
with the value of \tcode{desired}. Memory is affected according to the value of
\tcode{order}.
\end{itemdescr}

\indexlibrarymember{operator=}{atomic}%
\indexlibrarymember{operator=}{atomic<T*>}%
\indexlibrarymember{operator=}{atomic<\placeholder{integral}>}%
\indexlibrarymember{operator=}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
T operator=(T desired) volatile noexcept;
T operator=(T desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to \tcode{store(desired)}.

\pnum
\returns
\tcode{desired}.
\end{itemdescr}

\indexlibraryglobal{atomic_load}%
\indexlibraryglobal{atomic_load_explicit}%
\indexlibrarymember{load}{atomic}%
\indexlibrarymember{load}{atomic<T*>}%
\indexlibrarymember{load}{atomic<\placeholder{integral}>}%
\indexlibrarymember{load}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
T load(memory_order order = memory_order::seq_cst) const volatile noexcept;
T load(memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\expects
The \tcode{order} argument is neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Memory is affected according to the value of \tcode{order}.

\pnum
\returns
Atomically returns the value pointed to by \keyword{this}.
\end{itemdescr}

\indexlibrarymember{operator \placeholder{type}}{atomic}%
\indexlibrarymember{operator T*}{atomic<T*>}%
\indexlibrarymember{operator \placeholder{integral}}{atomic<\placeholder{integral}>}%
\indexlibrarymember{operator \placeholder{floating-point}}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
operator T() const volatile noexcept;
operator T() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return load();}
\end{itemdescr}


\indexlibraryglobal{atomic_exchange}%
\indexlibraryglobal{atomic_exchange_explicit}%
\indexlibrarymember{exchange}{atomic}%
\indexlibrarymember{exchange}{atomic<T*>}%
\indexlibrarymember{exchange}{atomic<\placeholder{integral}>}%
\indexlibrarymember{exchange}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
T exchange(T desired, memory_order order = memory_order::seq_cst) volatile noexcept;
T exchange(T desired, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Atomically replaces the value pointed to by \keyword{this}
with \tcode{desired}.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.multithread}.

\pnum
\returns
Atomically returns the value pointed to by \keyword{this} immediately before the effects.
\end{itemdescr}

\indexlibraryglobal{atomic_compare_exchange_weak}%
\indexlibraryglobal{atomic_compare_exchange_strong}%
\indexlibraryglobal{atomic_compare_exchange_weak_explicit}%
\indexlibraryglobal{atomic_compare_exchange_strong_explicit}%
\indexlibrarymember{compare_exchange_weak}{atomic}%
\indexlibrarymember{compare_exchange_weak}{atomic<T*>}%
\indexlibrarymember{compare_exchange_weak}{atomic<\placeholder{integral}>}%
\indexlibrarymember{compare_exchange_weak}{atomic<\placeholder{floating-point}>}%
\indexlibrarymember{compare_exchange_strong}{atomic}%
\indexlibrarymember{compare_exchange_strong}{atomic<T*>}%
\indexlibrarymember{compare_exchange_strong}{atomic<\placeholder{integral}>}%
\indexlibrarymember{compare_exchange_strong}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
bool compare_exchange_weak(T& expected, T desired,
                           memory_order success, memory_order failure) volatile noexcept;
bool compare_exchange_weak(T& expected, T desired,
                           memory_order success, memory_order failure) noexcept;
bool compare_exchange_strong(T& expected, T desired,
                             memory_order success, memory_order failure) volatile noexcept;
bool compare_exchange_strong(T& expected, T desired,
                             memory_order success, memory_order failure) noexcept;
bool compare_exchange_weak(T& expected, T desired,
                           memory_order order = memory_order::seq_cst) volatile noexcept;
bool compare_exchange_weak(T& expected, T desired,
                           memory_order order = memory_order::seq_cst) noexcept;
bool compare_exchange_strong(T& expected, T desired,
                             memory_order order = memory_order::seq_cst) volatile noexcept;
bool compare_exchange_strong(T& expected, T desired,
                             memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\expects
The \tcode{failure} argument is neither \tcode{memory_order::release} nor
\tcode{memory_order::acq_rel}.

\pnum
\effects
Retrieves the value in \tcode{expected}. It then atomically
compares the value representation of the value pointed to by \keyword{this}
for equality with that previously retrieved from \tcode{expected},
and if true, replaces the value pointed to
by \keyword{this} with that in \tcode{desired}.
If and only if the comparison is \tcode{true}, memory is affected according to the
value of \tcode{success}, and if the comparison is false, memory is affected according
to the value of \tcode{failure}. When only one \tcode{memory_order} argument is
supplied, the value of \tcode{success} is \tcode{order}, and the value of
\tcode{failure} is \tcode{order} except that a value of \tcode{memory_order::acq_rel}
shall be replaced by the value \tcode{memory_order::acquire} and a value of
\tcode{memory_order::release} shall be replaced by the value
\tcode{memory_order::relaxed}.
If and only if the comparison is false then, after the atomic operation,
the value in \tcode{expected} is replaced by the value
pointed to by \keyword{this} during the atomic comparison.
If the operation returns \tcode{true}, these
operations are atomic read-modify-write
operations\iref{intro.multithread} on the memory
pointed to by \keyword{this}.
Otherwise, these operations are atomic load operations on that memory.

\pnum
\returns
The result of the comparison.

\pnum
\begin{note}
For example, the effect of
\tcode{compare_exchange_strong}
on objects without padding bits\iref{basic.types} is
\begin{codeblock}
if (memcmp(this, &expected, sizeof(*this)) == 0)
  memcpy(this, &desired, sizeof(*this));
else
  memcpy(&expected, this, sizeof(*this));
\end{codeblock}
\end{note}
\begin{example}
The expected use of the compare-and-exchange operations is as follows. The
compare-and-exchange operations will update \tcode{expected} when another iteration of
the loop is needed.
\begin{codeblock}
expected = current.load();
do {
  desired = function(expected);
} while (!current.compare_exchange_weak(expected, desired));
\end{codeblock}
\end{example}
\begin{example}
Because the expected value is updated only on failure,
code releasing the memory containing the \tcode{expected} value on success will work.
For example, list head insertion will act atomically and would not introduce a
data race in the following code:
\begin{codeblock}
do {
  p->next = head;                                   // make new list node point to the current head
} while (!head.compare_exchange_weak(p->next, p));  // try to insert
\end{codeblock}
\end{example}

\pnum
Implementations should ensure that weak compare-and-exchange operations do not
consistently return \tcode{false} unless either the atomic object has value
different from \tcode{expected} or there are concurrent modifications to the
atomic object.

\pnum
\remarks
A weak compare-and-exchange operation may fail spuriously. That is, even when
the contents of memory referred to by \tcode{expected} and \keyword{this} are
equal, it may return \tcode{false} and store back to \tcode{expected} the same memory
contents that were originally there.
\begin{note}
This
spurious failure enables implementation of compare-and-exchange on a broader class of
machines, e.g., load-locked store-conditional machines. A
consequence of spurious failure is that nearly all uses of weak compare-and-exchange
will be in a loop.
When a compare-and-exchange is in a loop, the weak version will yield better performance
on some platforms. When a weak compare-and-exchange would require a loop and a strong one
would not, the strong one is preferable.
\end{note}

\pnum
\begin{note}
Under cases where the \tcode{memcpy} and \tcode{memcmp} semantics of the compare-and-exchange
operations apply, the comparisons can fail for values that compare equal with
\tcode{operator==} if the value representation has trap bits or alternate
representations of the same value. Notably, on implementations conforming to
ISO/IEC/IEEE 60559, floating-point \tcode{-0.0} and \tcode{+0.0}
will not compare equal with \tcode{memcmp} but will compare equal with \tcode{operator==},
and NaNs with the same payload will compare equal with \tcode{memcmp} but will not
compare equal with \tcode{operator==}.
\end{note}
\begin{note}
Because compare-and-exchange acts on an object's value representation,
padding bits that never participate in the object's value representation
are ignored. As a consequence, the following code is guaranteed to avoid
spurious failure:
\begin{codeblock}
struct padded {
  char clank = 0x42;
  // Padding here.
  unsigned biff = 0xC0DEFEFE;
};
atomic<padded> pad = {};

bool zap() {
  padded expected, desired{0, 0};
  return pad.compare_exchange_strong(expected, desired);
}
\end{codeblock}
\end{note}
\begin{note}
For a union with bits that participate in the value representation
of some members but not others, compare-and-exchange might always fail.
This is because such padding bits have an indeterminate value when they
do not participate in the value representation of the active member.
As a consequence, the following code is not guaranteed to ever succeed:
\begin{codeblock}
union pony {
  double celestia = 0.;
  short luna;       // padded
};
atomic<pony> princesses = {};

bool party(pony desired) {
  pony expected;
  return princesses.compare_exchange_strong(expected, desired);
}
\end{codeblock}
\end{note}
\end{itemdescr}

\indexlibrarymember{wait}{atomic}%
\indexlibrarymember{wait}{atomic<T*>}%
\indexlibrarymember{wait}{atomic<\placeholder{integral}>}%
\indexlibrarymember{wait}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
void wait(T old, memory_order order = memory_order::seq_cst) const volatile noexcept;
void wait(T old, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Repeatedly performs the following steps, in order:
\begin{itemize}
\item
  Evaluates \tcode{load(order)} and
  compares its value representation for equality against that of \tcode{old}.
\item
  If they compare unequal, returns.
\item
  Blocks until it
  is unblocked by an atomic notifying operation or is unblocked spuriously.
\end{itemize}

\pnum
\remarks
This function is an atomic waiting operation\iref{atomics.wait}.
\end{itemdescr}

\indexlibrarymember{notify_one}{atomic}%
\indexlibrarymember{notify_one}{atomic<T*>}%
\indexlibrarymember{notify_one}{atomic<\placeholder{integral}>}%
\indexlibrarymember{notify_one}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
void notify_one() volatile noexcept;
void notify_one() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of at least one atomic waiting operation
that is eligible to be unblocked\iref{atomics.wait} by this call,
if any such atomic waiting operations exist.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\indexlibrarymember{notify_all}{atomic}%
\indexlibrarymember{notify_all}{atomic<T*>}%
\indexlibrarymember{notify_all}{atomic<\placeholder{integral}>}%
\indexlibrarymember{notify_all}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
void notify_all() volatile noexcept;
void notify_all() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of all atomic waiting operations
that are eligible to be unblocked\iref{atomics.wait} by this call.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\rSec2[atomics.types.int]{Specializations for integers}

\indexlibrary{\idxcode{atomic<\placeholder{integral}>}}%
\pnum
There are specializations of the \tcode{atomic}
class template for the integral types
\tcode{char},
\tcode{signed char},
\tcode{unsigned char},
\tcode{short},
\tcode{unsigned short},
\tcode{int},
\tcode{unsigned int},
\tcode{long},
\tcode{unsigned long},
\tcode{long long},
\tcode{unsigned long long},
\keyword{char8_t},
\keyword{char16_t},
\keyword{char32_t},
\keyword{wchar_t},
and any other types needed by the typedefs in the header \libheaderref{cstdint}.
For each such type \tcode{\placeholder{integral}}, the specialization
\tcode{atomic<\placeholder{integral}>} provides additional atomic operations appropriate to integral types.
\begin{note}
The specialization \tcode{atomic<bool>}
uses the primary template\iref{atomics.types.generic}.
\end{note}

\begin{codeblock}
namespace std {
  template<> struct atomic<@\placeholder{integral}@> {
    using value_type = @\placeholder{integral}@;
    using difference_type = value_type;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;

    constexpr atomic() noexcept;
    constexpr atomic(@\placeholdernc{integral}@) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;

    void store(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    void store(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{integral}@ operator=(@\placeholdernc{integral}@) volatile noexcept;
    @\placeholdernc{integral}@ operator=(@\placeholdernc{integral}@) noexcept;
    @\placeholdernc{integral}@ load(memory_order = memory_order::seq_cst) const volatile noexcept;
    @\placeholdernc{integral}@ load(memory_order = memory_order::seq_cst) const noexcept;
    operator @\placeholdernc{integral}@() const volatile noexcept;
    operator @\placeholdernc{integral}@() const noexcept;

    @\placeholdernc{integral}@ exchange(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{integral}@ exchange(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                               memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                               memory_order, memory_order) noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                                 memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                                 memory_order, memory_order) noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                               memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_weak(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                               memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                                 memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_strong(@\placeholder{integral}@&, @\placeholdernc{integral}@,
                                 memory_order = memory_order::seq_cst) noexcept;

    @\placeholdernc{integral}@ fetch_add(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{integral}@ fetch_add(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{integral}@ fetch_sub(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{integral}@ fetch_sub(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{integral}@ fetch_and(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{integral}@ fetch_and(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{integral}@ fetch_or(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{integral}@ fetch_or(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{integral}@ fetch_xor(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{integral}@ fetch_xor(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) noexcept;

    @\placeholdernc{integral}@ operator++(int) volatile noexcept;
    @\placeholdernc{integral}@ operator++(int) noexcept;
    @\placeholdernc{integral}@ operator--(int) volatile noexcept;
    @\placeholdernc{integral}@ operator--(int) noexcept;
    @\placeholdernc{integral}@ operator++() volatile noexcept;
    @\placeholdernc{integral}@ operator++() noexcept;
    @\placeholdernc{integral}@ operator--() volatile noexcept;
    @\placeholdernc{integral}@ operator--() noexcept;
    @\placeholdernc{integral}@ operator+=(@\placeholdernc{integral}@) volatile noexcept;
    @\placeholdernc{integral}@ operator+=(@\placeholdernc{integral}@) noexcept;
    @\placeholdernc{integral}@ operator-=(@\placeholdernc{integral}@) volatile noexcept;
    @\placeholdernc{integral}@ operator-=(@\placeholdernc{integral}@) noexcept;
    @\placeholdernc{integral}@ operator&=(@\placeholdernc{integral}@) volatile noexcept;
    @\placeholdernc{integral}@ operator&=(@\placeholdernc{integral}@) noexcept;
    @\placeholdernc{integral}@ operator|=(@\placeholdernc{integral}@) volatile noexcept;
    @\placeholdernc{integral}@ operator|=(@\placeholdernc{integral}@) noexcept;
    @\placeholdernc{integral}@ operator^=(@\placeholdernc{integral}@) volatile noexcept;
    @\placeholdernc{integral}@ operator^=(@\placeholdernc{integral}@) noexcept;

    void wait(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) const volatile noexcept;
    void wait(@\placeholdernc{integral}@, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() volatile noexcept;
    void notify_one() noexcept;
    void notify_all() volatile noexcept;
    void notify_all() noexcept;
  };
}
\end{codeblock}

\pnum
The atomic integral specializations
are standard-layout structs.
They each have
a trivial destructor.

\pnum
Descriptions are provided below only for members that differ from the primary template.

\pnum
The following operations perform arithmetic computations. The key, operator, and computation correspondence is:

\begin{floattable}
{Atomic arithmetic computations}{atomic.types.int.comp}{lll|lll}
\hline
\hdstyle{\tcode{\placeholder{key}}}   &
  \hdstyle{Op}                        &
  \hdstyle{Computation}               &
\hdstyle{\tcode{\placeholder{key}}}   &
  \hdstyle{Op}                        &
  \hdstyle{Computation}  \\ \hline
\tcode{add}       &
  \tcode{+}       &
  addition        &
\tcode{sub}       &
  \tcode{-}       &
  subtraction     \\
\tcode{or}        &
  \tcode{|}       &
  bitwise inclusive or  &
\tcode{xor}       &
  \tcode{\caret}        &
  bitwise exclusive or  \\
\tcode{and}       &
  \tcode{\&}      &
  bitwise and     &&&\\
\end{floattable}

\indexlibraryglobal{atomic_fetch_add}%
\indexlibraryglobal{atomic_fetch_and}%
\indexlibraryglobal{atomic_fetch_or}%
\indexlibraryglobal{atomic_fetch_sub}%
\indexlibraryglobal{atomic_fetch_xor}%
\indexlibraryglobal{atomic_fetch_add_explicit}%
\indexlibraryglobal{atomic_fetch_and_explicit}%
\indexlibraryglobal{atomic_fetch_or_explicit}%
\indexlibraryglobal{atomic_fetch_sub_explicit}%
\indexlibraryglobal{atomic_fetch_xor_explicit}%
\indexlibrarymember{fetch_add}{atomic<\placeholder{integral}>}%
\indexlibrarymember{fetch_and}{atomic<\placeholder{integral}>}%
\indexlibrarymember{fetch_or}{atomic<\placeholder{integral}>}%
\indexlibrarymember{fetch_sub}{atomic<\placeholder{integral}>}%
\indexlibrarymember{fetch_xor}{atomic<\placeholder{integral}>}%
\begin{itemdecl}
T fetch_@\placeholdernc{key}@(T operand, memory_order order = memory_order::seq_cst) volatile noexcept;
T fetch_@\placeholdernc{key}@(T operand, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Atomically replaces the value pointed to by
\keyword{this} with the result of the computation applied to the
value pointed to by \keyword{this} and the given \tcode{operand}.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.multithread}.

\pnum
\returns
Atomically, the value pointed to by \keyword{this} immediately before the effects.

\pnum
\indextext{signed integer representation!two's complement}%
\remarks
For signed integer types,
the result is as if the object value and parameters
were converted to their corresponding unsigned types,
the computation performed on those types, and
the result converted back to the signed type.
\begin{note}
There are no undefined results arising from the computation.
\end{note}

\end{itemdescr}

\indexlibrarymember{operator+=}{atomic<T*>}%
\indexlibrarymember{operator-=}{atomic<T*>}%
\indexlibrarymember{operator+=}{atomic<\placeholder{integral}>}%
\indexlibrarymember{operator-=}{atomic<\placeholder{integral}>}%
\indexlibrarymember{operator\&=}{atomic<\placeholder{integral}>}%
\indexlibrarymember{operator"|=}{atomic<\placeholder{integral}>}%
\indexlibrarymember{operator\caret=}{atomic<\placeholder{integral}>}%
\begin{itemdecl}
T operator @\placeholder{op}@=(T operand) volatile noexcept;
T operator @\placeholder{op}@=(T operand) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_\placeholder{key}(operand) \placeholder{op} operand;}
\end{itemdescr}

\rSec2[atomics.types.float]{Specializations for floating-point types}

\indexlibrary{\idxcode{atomic<\placeholder{floating-point}>}}%
\pnum
There are specializations of the \tcode{atomic}
class template for the floating-point types
\tcode{float},
\tcode{double}, and
\tcode{long double}.
For each such type \tcode{\placeholdernc{floating-point}},
the specialization \tcode{atomic<\placeholder{floating-point}>}
provides additional atomic operations appropriate to floating-point types.

\begin{codeblock}
namespace std {
  template<> struct atomic<@\placeholder{floating-point}@> {
    using value_type = @\placeholdernc{floating-point}@;
    using difference_type = value_type;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;

    constexpr atomic() noexcept;
    constexpr atomic(@\placeholder{floating-point}@) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;

    void store(@\placeholdernc{floating-point}@, memory_order = memory_order::seq_cst) volatile noexcept;
    void store(@\placeholdernc{floating-point}@, memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{floating-point}@ operator=(@\placeholder{floating-point}@) volatile noexcept;
    @\placeholdernc{floating-point}@ operator=(@\placeholder{floating-point}@) noexcept;
    @\placeholdernc{floating-point}@ load(memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{floating-point}@ load(memory_order = memory_order::seq_cst) noexcept;
    operator @\placeholdernc{floating-point}@() volatile noexcept;
    operator @\placeholdernc{floating-point}@() noexcept;

    @\placeholdernc{floating-point}@ exchange(@\placeholdernc{floating-point}@,
                            memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{floating-point}@ exchange(@\placeholdernc{floating-point}@,
                            memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                               memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                               memory_order, memory_order) noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                                 memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                                 memory_order, memory_order) noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                               memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_weak(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                               memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                                 memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_strong(@\placeholder{floating-point}@&, @\placeholdernc{floating-point}@,
                                 memory_order = memory_order::seq_cst) noexcept;

    @\placeholdernc{floating-point}@ fetch_add(@\placeholdernc{floating-point}@,
                             memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{floating-point}@ fetch_add(@\placeholdernc{floating-point}@,
                             memory_order = memory_order::seq_cst) noexcept;
    @\placeholdernc{floating-point}@ fetch_sub(@\placeholdernc{floating-point}@,
                             memory_order = memory_order::seq_cst) volatile noexcept;
    @\placeholdernc{floating-point}@ fetch_sub(@\placeholdernc{floating-point}@,
                             memory_order = memory_order::seq_cst) noexcept;

    @\placeholdernc{floating-point}@ operator+=(@\placeholder{floating-point}@) volatile noexcept;
    @\placeholdernc{floating-point}@ operator+=(@\placeholder{floating-point}@) noexcept;
    @\placeholdernc{floating-point}@ operator-=(@\placeholder{floating-point}@) volatile noexcept;
    @\placeholdernc{floating-point}@ operator-=(@\placeholder{floating-point}@) noexcept;

    void wait(@\placeholdernc{floating-point}@, memory_order = memory_order::seq_cst) const volatile noexcept;
    void wait(@\placeholdernc{floating-point}@, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() volatile noexcept;
    void notify_one() noexcept;
    void notify_all() volatile noexcept;
    void notify_all() noexcept;
  };
}
\end{codeblock}

\pnum
The atomic floating-point specializations
are standard-layout structs.
They each have
a trivial destructor.

\pnum
Descriptions are provided below only for members that differ from the primary template.

\pnum
The following operations perform arithmetic addition and subtraction computations.
The key, operator, and computation correspondence are identified in
\tref{atomic.types.int.comp}.

\indexlibraryglobal{atomic_fetch_add}%
\indexlibraryglobal{atomic_fetch_sub}%
\indexlibraryglobal{atomic_fetch_add_explicit}%
\indexlibraryglobal{atomic_fetch_sub_explicit}%
\indexlibrarymember{fetch_add}{atomic<\placeholder{floating-point}>}%
\indexlibrarymember{fetch_sub}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
T fetch_@\placeholdernc{key}@(T operand, memory_order order = memory_order::seq_cst) volatile noexcept;
T fetch_@\placeholdernc{key}@(T operand, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Atomically replaces the value pointed to by \keyword{this}
with the result of the computation applied to the value pointed
to by \keyword{this} and the given \tcode{operand}.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.multithread}.

\pnum
\returns
Atomically, the value pointed to by \keyword{this} immediately before the effects.

\pnum
\remarks
If the result is not a representable value for its type\iref{expr.pre}
the result is unspecified, but the operations otherwise have no undefined
behavior. Atomic arithmetic operations on \tcode{\placeholder{floating-point}}
should conform to the \tcode{std::numeric_limits<\placeholder{floating-point}>}
traits associated with the floating-point type\iref{limits.syn}.
The floating-point environment\iref{cfenv} for atomic arithmetic operations
on \tcode{\placeholder{floating-point}} may be different than the
calling thread's floating-point environment.
\end{itemdescr}

\indexlibrarymember{operator+=}{atomic<T*>}%
\indexlibrarymember{operator-=}{atomic<T*>}%
\indexlibrarymember{operator+=}{atomic<\placeholder{floating-point}>}%
\indexlibrarymember{operator-=}{atomic<\placeholder{floating-point}>}%
\begin{itemdecl}
T operator @\placeholder{op}@=(T operand) volatile noexcept;
T operator @\placeholder{op}@=(T operand) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_\placeholder{key}(operand) \placeholder{op} operand;}

\pnum
\remarks
If the result is not a representable value for its type\iref{expr.pre}
the result is unspecified, but the operations otherwise have no undefined
behavior. Atomic arithmetic operations on \tcode{\placeholder{floating-point}}
should conform to the \tcode{std::numeric_limits<\placeholder{floating-point}>}
traits associated with the floating-point type\iref{limits.syn}.
The floating-point environment\iref{cfenv} for atomic arithmetic operations
on \tcode{\placeholder{floating-point}} may be different than the
calling thread's floating-point environment.
\end{itemdescr}

\rSec2[atomics.types.pointer]{Partial specialization for pointers}
\indexlibraryglobal{atomic<T*>}%

\begin{codeblock}
namespace std {
  template<class T> struct atomic<T*> {
    using value_type = T*;
    using difference_type = ptrdiff_t;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
    bool is_lock_free() const volatile noexcept;
    bool is_lock_free() const noexcept;

    constexpr atomic() noexcept;
    constexpr atomic(T*) noexcept;
    atomic(const atomic&) = delete;
    atomic& operator=(const atomic&) = delete;
    atomic& operator=(const atomic&) volatile = delete;

    void store(T*, memory_order = memory_order::seq_cst) volatile noexcept;
    void store(T*, memory_order = memory_order::seq_cst) noexcept;
    T* operator=(T*) volatile noexcept;
    T* operator=(T*) noexcept;
    T* load(memory_order = memory_order::seq_cst) const volatile noexcept;
    T* load(memory_order = memory_order::seq_cst) const noexcept;
    operator T*() const volatile noexcept;
    operator T*() const noexcept;

    T* exchange(T*, memory_order = memory_order::seq_cst) volatile noexcept;
    T* exchange(T*, memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_weak(T*&, T*, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_weak(T*&, T*, memory_order, memory_order) noexcept;
    bool compare_exchange_strong(T*&, T*, memory_order, memory_order) volatile noexcept;
    bool compare_exchange_strong(T*&, T*, memory_order, memory_order) noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_weak(T*&, T*,
                               memory_order = memory_order::seq_cst) noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order = memory_order::seq_cst) volatile noexcept;
    bool compare_exchange_strong(T*&, T*,
                                 memory_order = memory_order::seq_cst) noexcept;

    T* fetch_add(ptrdiff_t, memory_order = memory_order::seq_cst) volatile noexcept;
    T* fetch_add(ptrdiff_t, memory_order = memory_order::seq_cst) noexcept;
    T* fetch_sub(ptrdiff_t, memory_order = memory_order::seq_cst) volatile noexcept;
    T* fetch_sub(ptrdiff_t, memory_order = memory_order::seq_cst) noexcept;

    T* operator++(int) volatile noexcept;
    T* operator++(int) noexcept;
    T* operator--(int) volatile noexcept;
    T* operator--(int) noexcept;
    T* operator++() volatile noexcept;
    T* operator++() noexcept;
    T* operator--() volatile noexcept;
    T* operator--() noexcept;
    T* operator+=(ptrdiff_t) volatile noexcept;
    T* operator+=(ptrdiff_t) noexcept;
    T* operator-=(ptrdiff_t) volatile noexcept;
    T* operator-=(ptrdiff_t) noexcept;

    void wait(T*, memory_order = memory_order::seq_cst) const volatile noexcept;
    void wait(T*, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() volatile noexcept;
    void notify_one() noexcept;
    void notify_all() volatile noexcept;
    void notify_all() noexcept;
  };
}
\end{codeblock}

\indexlibraryglobal{atomic<T*>}%
\pnum
There is a partial specialization of the \tcode{atomic} class template for pointers.
Specializations of this partial specialization are standard-layout structs.
They each have a trivial destructor.

\pnum
Descriptions are provided below only for members that differ from the primary template.

\pnum
The following operations perform pointer arithmetic. The key, operator,
and computation correspondence is:

\begin{floattable}
{Atomic pointer computations}{atomic.types.pointer.comp}{lll|lll}
\hline
\hdstyle{\tcode{\placeholder{key}}}  &
  \hdstyle{Op}                       &
  \hdstyle{Computation}              &
\hdstyle{\tcode{\placeholder{key}}}  &
  \hdstyle{Op}                       &
  \hdstyle{Computation}  \\ \hline
\tcode{add}       &
  \tcode{+}       &
  addition        &
\tcode{sub}       &
  \tcode{-}       &
  subtraction     \\
\end{floattable}

\indexlibraryglobal{atomic_fetch_add}%
\indexlibraryglobal{atomic_fetch_sub}%
\indexlibraryglobal{atomic_fetch_add_explicit}%
\indexlibraryglobal{atomic_fetch_sub_explicit}%
\indexlibrarymember{fetch_add}{atomic<T*>}%
\indexlibrarymember{fetch_sub}{atomic<T*>}%
\begin{itemdecl}
T* fetch_@\placeholdernc{key}@(ptrdiff_t operand, memory_order order = memory_order::seq_cst) volatile noexcept;
T* fetch_@\placeholdernc{key}@(ptrdiff_t operand, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\mandates
\tcode{T} is a complete object type.
\begin{note}
Pointer arithmetic on \tcode{void*} or function pointers is ill-formed.
\end{note}

\pnum
\effects
Atomically replaces the value pointed to by
\keyword{this} with the result of the computation applied to the
value pointed to by \keyword{this} and the given \tcode{operand}.
Memory is affected according to the value of \tcode{order}.
These operations are atomic read-modify-write operations\iref{intro.multithread}.

\pnum
\returns
Atomically, the value pointed to by \keyword{this} immediately before the effects.

\pnum
\remarks
The result may be an undefined address,
but the operations otherwise have no undefined behavior.
\end{itemdescr}

\indexlibrarymember{operator+=}{atomic<T*>}%
\indexlibrarymember{operator-=}{atomic<T*>}%
\begin{itemdecl}
T* operator @\placeholder{op}@=(ptrdiff_t operand) volatile noexcept;
T* operator @\placeholder{op}@=(ptrdiff_t operand) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_\placeholder{key}(operand) \placeholder{op} operand;}
\end{itemdescr}

\rSec2[atomics.types.memop]{Member operators common to integers and pointers to objects}

\indexlibrarymember{operator++}{atomic<T*>}%
\indexlibrarymember{operator++}{atomic<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator++(int) volatile noexcept;
value_type operator++(int) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_add(1);}
\end{itemdescr}

\indexlibrarymember{operator--}{atomic<T*>}%
\indexlibrarymember{operator--}{atomic<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator--(int) volatile noexcept;
value_type operator--(int) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_sub(1);}
\end{itemdescr}

\indexlibrarymember{operator++}{atomic<T*>}%
\indexlibrarymember{operator++}{atomic<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator++() volatile noexcept;
value_type operator++() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_add(1) + 1;}
\end{itemdescr}

\indexlibrarymember{operator--}{atomic<T*>}%
\indexlibrarymember{operator--}{atomic<\placeholder{integral}>}%
\begin{itemdecl}
value_type operator--() volatile noexcept;
value_type operator--() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\constraints
For the \tcode{volatile} overload of this function,
\tcode{is_always_lock_free} is \tcode{true}.

\pnum
\effects
Equivalent to: \tcode{return fetch_sub(1) - 1;}
\end{itemdescr}

\rSec2[util.smartptr.atomic]{Partial specializations for smart pointers}%
\indextext{atomic!smart pointers|(}%

\rSec3[util.smartptr.atomic.general]{General}

\pnum
The library provides partial specializations of the \tcode{atomic} template
for shared-ownership smart pointers\iref{smartptr}.
The behavior of all operations is as specified in \ref{atomics.types.generic},
unless specified otherwise.
The template parameter \tcode{T} of these partial specializations
may be an incomplete type.

\pnum
All changes to an atomic smart pointer in \ref{util.smartptr.atomic}, and
all associated \tcode{use_count} increments,
are guaranteed to be performed atomically.
Associated \tcode{use_count} decrements
are sequenced after the atomic operation,
but are not required to be part of it.
Any associated deletion and deallocation
are sequenced after the atomic update step and
are not part of the atomic operation.
\begin{note}
If the atomic operation uses locks,
locks acquired by the implementation
will be held when any \tcode{use_count} adjustments are performed, and
will not be held when any destruction or deallocation
resulting from this is performed.
\end{note}

\pnum
\begin{example}
\begin{codeblock}
template<typename T> class atomic_list {
  struct node {
    T t;
    shared_ptr<node> next;
  };
  atomic<shared_ptr<node>> head;

public:
  auto find(T t) const {
    auto p = head.load();
    while (p && p->t != t)
      p = p->next;

    return shared_ptr<node>(move(p));
  }

  void push_front(T t) {
    auto p = make_shared<node>();
    p->t = t;
    p->next = head;
    while (!head.compare_exchange_weak(p->next, p)) {}
  }
};
\end{codeblock}
\end{example}

\rSec3[util.smartptr.atomic.shared]{Partial specialization for \tcode{shared_ptr}}
\indexlibraryglobal{atomic<shared_ptr<T>>}%
\begin{codeblock}
namespace std {
  template<class T> struct atomic<shared_ptr<T>> {
    using value_type = shared_ptr<T>;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
    bool is_lock_free() const noexcept;

    constexpr atomic() noexcept;
    atomic(shared_ptr<T> desired) noexcept;
    atomic(const atomic&) = delete;
    void operator=(const atomic&) = delete;

    shared_ptr<T> load(memory_order order = memory_order::seq_cst) const noexcept;
    operator shared_ptr<T>() const noexcept;
    void store(shared_ptr<T> desired, memory_order order = memory_order::seq_cst) noexcept;
    void operator=(shared_ptr<T> desired) noexcept;

    shared_ptr<T> exchange(shared_ptr<T> desired,
                           memory_order order = memory_order::seq_cst) noexcept;
    bool compare_exchange_weak(shared_ptr<T>& expected, shared_ptr<T> desired,
                               memory_order success, memory_order failure) noexcept;
    bool compare_exchange_strong(shared_ptr<T>& expected, shared_ptr<T> desired,
                                 memory_order success, memory_order failure) noexcept;
    bool compare_exchange_weak(shared_ptr<T>& expected, shared_ptr<T> desired,
                               memory_order order = memory_order::seq_cst) noexcept;
    bool compare_exchange_strong(shared_ptr<T>& expected, shared_ptr<T> desired,
                                 memory_order order = memory_order::seq_cst) noexcept;

    void wait(shared_ptr<T> old, memory_order order = memory_order::seq_cst) const noexcept;
    void notify_one() noexcept;
    void notify_all() noexcept;

  private:
    shared_ptr<T> p;            // \expos
  };
}
\end{codeblock}

\indexlibraryctor{atomic<shared_ptr<T>>}%
\begin{itemdecl}
constexpr atomic() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Initializes \tcode{p\{\}}.
\end{itemdescr}

\indexlibraryctor{atomic<shared_ptr<T>>}%
\begin{itemdecl}
atomic(shared_ptr<T> desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Initializes the object with the value \tcode{desired}.
Initialization is not an atomic operation\iref{intro.multithread}.
\begin{note}
It is possible to have an access to
an atomic object \tcode{A} race with its construction,
for example,
by communicating the address of the just-constructed object \tcode{A}
to another thread via \tcode{memory_order::relaxed} operations
on a suitable atomic pointer variable, and
then immediately accessing \tcode{A} in the receiving thread.
This results in undefined behavior.
\end{note}
\end{itemdescr}

\indexlibrarymember{store}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
void store(shared_ptr<T> desired, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is neither
\tcode{memory_order::consume},
\tcode{memory_order::acquire}, nor
\tcode{memory_order::acq_rel}.

\pnum
\effects
Atomically replaces the value pointed to by \keyword{this} with
the value of \tcode{desired} as if by \tcode{p.swap(desired)}.
Memory is affected according to the value of \tcode{order}.
\end{itemdescr}

\indexlibrarymember{operator=}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
void operator=(shared_ptr<T> desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to \tcode{store(desired)}.
\end{itemdescr}

\indexlibrarymember{load}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
shared_ptr<T> load(memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is neither
\tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Memory is affected according to the value of \tcode{order}.

\pnum
\returns
Atomically returns \tcode{p}.
\end{itemdescr}

\indexlibrarymember{operator shared_ptr<T>}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
operator shared_ptr<T>() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return load();}
\end{itemdescr}

\indexlibrarymember{exchange}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
shared_ptr<T> exchange(shared_ptr<T> desired, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Atomically replaces \tcode{p} with \tcode{desired}
as if by \tcode{p.swap(desired)}.
Memory is affected according to the value of \tcode{order}.
This is an atomic read-modify-write operation\iref{intro.races}.

\pnum
\returns
Atomically returns the value of \tcode{p} immediately before the effects.
\end{itemdescr}

\indexlibrarymember{compare_exchange_weak}{atomic<shared_ptr<T>>}%
\indexlibrarymember{compare_exchange_strong}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
bool compare_exchange_weak(shared_ptr<T>& expected, shared_ptr<T> desired,
                           memory_order success, memory_order failure) noexcept;
bool compare_exchange_strong(shared_ptr<T>& expected, shared_ptr<T> desired,
                             memory_order success, memory_order failure) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{failure} is neither
\tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
If \tcode{p} is equivalent to \tcode{expected},
assigns \tcode{desired} to \tcode{p} and
has synchronization semantics corresponding to the value of \tcode{success},
otherwise assigns \tcode{p} to \tcode{expected} and
has synchronization semantics corresponding to the value of \tcode{failure}.

\pnum
\returns
\tcode{true} if \tcode{p} was equivalent to \tcode{expected},
\tcode{false} otherwise.

\pnum
\remarks
Two \tcode{shared_ptr} objects are equivalent if
they store the same pointer value and
either share ownership or are both empty.
The weak form may fail spuriously. See \ref{atomics.types.operations}.

\pnum
If the operation returns \tcode{true},
\tcode{expected} is not accessed after the atomic update and
the operation is an atomic read-modify-write operation\iref{intro.multithread}
on the memory pointed to by \keyword{this}.
Otherwise, the operation is an atomic load operation on that memory, and
\tcode{expected} is updated with the existing value
read from the atomic object in the attempted atomic update.
The \tcode{use_count} update corresponding to the write to \tcode{expected}
is part of the atomic operation.
The write to \tcode{expected} itself
is not required to be part of the atomic operation.
\end{itemdescr}

\indexlibrarymember{compare_exchange_weak}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
bool compare_exchange_weak(shared_ptr<T>& expected, shared_ptr<T> desired,
                           memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\begin{codeblock}
return compare_exchange_weak(expected, desired, order, fail_order);
\end{codeblock}
where \tcode{fail_order} is the same as \tcode{order}
except that a value of \tcode{memory_order::acq_rel}
shall be replaced by the value \tcode{memory_order::acquire} and
a value of \tcode{memory_order::release}
shall be replaced by the value \tcode{memory_order::relaxed}.
\end{itemdescr}

\indexlibrarymember{compare_exchange_strong}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
bool compare_exchange_strong(shared_ptr<T>& expected, shared_ptr<T> desired,
                             memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\begin{codeblock}
return compare_exchange_strong(expected, desired, order, fail_order);
\end{codeblock}
where \tcode{fail_order} is the same as \tcode{order}
except that a value of \tcode{memory_order::acq_rel}
shall be replaced by the value \tcode{memory_order::acquire} and
a value of \tcode{memory_order::release}
shall be replaced by the value \tcode{memory_order::relaxed}.
\end{itemdescr}

\indexlibrarymember{wait}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
void wait(shared_ptr<T> old, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is
neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Repeatedly performs the following steps, in order:
\begin{itemize}
\item
  Evaluates \tcode{load(order)} and compares it to \tcode{old}.
\item
  If the two are not equivalent, returns.
\item
  Blocks until it
  is unblocked by an atomic notifying operation or is unblocked spuriously.
\end{itemize}

\pnum
\remarks
Two \tcode{shared_ptr} objects are equivalent
if they store the same pointer and either share ownership or are both empty.
This function is an atomic waiting operation\iref{atomics.wait}.
\end{itemdescr}

\indexlibrarymember{notify_one}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
void notify_one() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of at least one atomic waiting operation
that is eligible to be unblocked\iref{atomics.wait} by this call,
if any such atomic waiting operations exist.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\indexlibrarymember{notify_all}{atomic<shared_ptr<T>>}%
\begin{itemdecl}
void notify_all() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of all atomic waiting operations
that are eligible to be unblocked\iref{atomics.wait} by this call.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\rSec3[util.smartptr.atomic.weak]{Partial specialization for \tcode{weak_ptr}}
\indexlibraryglobal{atomic<weak_ptr<T>>}%
\begin{codeblock}
namespace std {
  template<class T> struct atomic<weak_ptr<T>> {
    using value_type = weak_ptr<T>;

    static constexpr bool is_always_lock_free = @\impdefx{whether a given \tcode{atomic} type's operations are always lock free}@;
    bool is_lock_free() const noexcept;

    constexpr atomic() noexcept;
    atomic(weak_ptr<T> desired) noexcept;
    atomic(const atomic&) = delete;
    void operator=(const atomic&) = delete;

    weak_ptr<T> load(memory_order order = memory_order::seq_cst) const noexcept;
    operator weak_ptr<T>() const noexcept;
    void store(weak_ptr<T> desired, memory_order order = memory_order::seq_cst) noexcept;
    void operator=(weak_ptr<T> desired) noexcept;

    weak_ptr<T> exchange(weak_ptr<T> desired,
                         memory_order order = memory_order::seq_cst) noexcept;
    bool compare_exchange_weak(weak_ptr<T>& expected, weak_ptr<T> desired,
                               memory_order success, memory_order failure) noexcept;
    bool compare_exchange_strong(weak_ptr<T>& expected, weak_ptr<T> desired,
                                 memory_order success, memory_order failure) noexcept;
    bool compare_exchange_weak(weak_ptr<T>& expected, weak_ptr<T> desired,
                               memory_order order = memory_order::seq_cst) noexcept;
    bool compare_exchange_strong(weak_ptr<T>& expected, weak_ptr<T> desired,
                                 memory_order order = memory_order::seq_cst) noexcept;

    void wait(weak_ptr<T> old, memory_order order = memory_order::seq_cst) const noexcept;
    void notify_one() noexcept;
    void notify_all() noexcept;

  private:
    weak_ptr<T> p;              // \expos
  };
}
\end{codeblock}

\indexlibraryctor{atomic<weak_ptr<T>>}%
\begin{itemdecl}
constexpr atomic() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Initializes \tcode{p\{\}}.
\end{itemdescr}

\indexlibraryctor{atomic<weak_ptr<T>>}%
\begin{itemdecl}
atomic(weak_ptr<T> desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Initializes the object with the value \tcode{desired}.
Initialization is not an atomic operation\iref{intro.multithread}.
\begin{note}
It is possible to have an access to
an atomic object \tcode{A} race with its construction,
for example,
by communicating the address of the just-constructed object \tcode{A}
to another thread via \tcode{memory_order::relaxed} operations
on a suitable atomic pointer variable, and
then immediately accessing \tcode{A} in the receiving thread.
This results in undefined behavior.
\end{note}
\end{itemdescr}

\indexlibrarymember{store}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
void store(weak_ptr<T> desired, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is neither
\tcode{memory_order::consume},
\tcode{memory_order::acquire}, nor
\tcode{memory_order::acq_rel}.

\pnum
\effects
Atomically replaces the value pointed to by \keyword{this} with
the value of \tcode{desired} as if by \tcode{p.swap(desired)}.
Memory is affected according to the value of \tcode{order}.
\end{itemdescr}

\indexlibrarymember{operator=}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
void operator=(weak_ptr<T> desired) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to \tcode{store(desired)}.
\end{itemdescr}

\indexlibrarymember{load}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
weak_ptr<T> load(memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is neither
\tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Memory is affected according to the value of \tcode{order}.

\pnum
\returns
Atomically returns \tcode{p}.
\end{itemdescr}

\indexlibrarymember{operator weak_ptr<T>}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
operator weak_ptr<T>() const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to: \tcode{return load();}
\end{itemdescr}

\indexlibrarymember{exchange}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
weak_ptr<T> exchange(weak_ptr<T> desired, memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Atomically replaces \tcode{p} with \tcode{desired}
as if by \tcode{p.swap(desired)}.
Memory is affected according to the value of \tcode{order}.
This is an atomic read-modify-write operation\iref{intro.races}.

\pnum
\returns
Atomically returns the value of \tcode{p} immediately before the effects.
\end{itemdescr}

\indexlibrarymember{compare_exchange_weak}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
bool compare_exchange_weak(weak_ptr<T>& expected, weak_ptr<T> desired,
                           memory_order success, memory_order failure) noexcept;
bool compare_exchange_strong(weak_ptr<T>& expected, weak_ptr<T> desired,
                             memory_order success, memory_order failure) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{failure} is neither
\tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
If \tcode{p} is equivalent to \tcode{expected},
assigns \tcode{desired} to \tcode{p} and
has synchronization semantics corresponding to the value of \tcode{success},
otherwise assigns \tcode{p} to \tcode{expected} and
has synchronization semantics corresponding to the value of \tcode{failure}.

\pnum
\returns
\tcode{true} if \tcode{p} was equivalent to \tcode{expected},
\tcode{false} otherwise.

\pnum
\remarks
Two \tcode{weak_ptr} objects are equivalent if
they store the same pointer value and
either share ownership or are both empty.
The weak form may fail spuriously. See \ref{atomics.types.operations}.

\pnum
If the operation returns \tcode{true},
\tcode{expected} is not accessed after the atomic update and
the operation is an atomic read-modify-write operation\iref{intro.multithread}
on the memory pointed to by \keyword{this}.
Otherwise, the operation is an atomic load operation on that memory, and
\tcode{expected} is updated with the existing value
read from the atomic object in the attempted atomic update.
The \tcode{use_count} update corresponding to the write to \tcode{expected}
is part of the atomic operation.
The write to \tcode{expected} itself
is not required to be part of the atomic operation.
\end{itemdescr}

\indexlibrarymember{compare_exchange_weak}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
bool compare_exchange_weak(weak_ptr<T>& expected, weak_ptr<T> desired,
                           memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\begin{codeblock}
return compare_exchange_weak(expected, desired, order, fail_order);
\end{codeblock}
where \tcode{fail_order} is the same as \tcode{order}
except that a value of \tcode{memory_order::acq_rel}
shall be replaced by the value \tcode{memory_order::acquire} and
a value of \tcode{memory_order::release}
shall be replaced by the value \tcode{memory_order::relaxed}.
\end{itemdescr}

\indexlibrarymember{compare_exchange_strong}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
bool compare_exchange_strong(weak_ptr<T>& expected, weak_ptr<T> desired,
                             memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to:
\begin{codeblock}
return compare_exchange_strong(expected, desired, order, fail_order);
\end{codeblock}
where \tcode{fail_order} is the same as \tcode{order}
except that a value of \tcode{memory_order::acq_rel}
shall be replaced by the value \tcode{memory_order::acquire} and
a value of \tcode{memory_order::release}
shall be replaced by the value \tcode{memory_order::relaxed}.
\end{itemdescr}

\indexlibrarymember{wait}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
void wait(weak_ptr<T> old, memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
\tcode{order} is
neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Repeatedly performs the following steps, in order:
\begin{itemize}
\item
  Evaluates \tcode{load(order)} and compares it to \tcode{old}.
\item
  If the two are not equivalent, returns.
\item
  Blocks until it
  is unblocked by an atomic notifying operation or is unblocked spuriously.
\end{itemize}

\pnum
\remarks
Two \tcode{weak_ptr} objects are equivalent
if they store the same pointer and either share ownership or are both empty.
This function is an atomic waiting operation\iref{atomics.wait}.
\end{itemdescr}


\indexlibrarymember{notify_one}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
void notify_one() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of at least one atomic waiting operation
that is eligible to be unblocked\iref{atomics.wait} by this call,
if any such atomic waiting operations exist.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\indexlibrarymember{notify_all}{atomic<weak_ptr<T>>}%
\begin{itemdecl}
void notify_all() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of all atomic waiting operations
that are eligible to be unblocked\iref{atomics.wait} by this call.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}
\indextext{atomic!smart pointers|)}

\rSec1[atomics.nonmembers]{Non-member functions}

\pnum
A non-member function template whose name matches the pattern
\tcode{atomic_\placeholder{f}} or the pattern \tcode{atomic_\placeholder{f}_explicit}
invokes the member function \tcode{\placeholder{f}}, with the value of the
first parameter as the object expression and the values of the remaining parameters
(if any) as the arguments of the member function call, in order. An argument
for a parameter of type \tcode{atomic<T>::value_type*} is dereferenced when
passed to the member function call.
If no such member function exists, the program is ill-formed.

\pnum
\begin{note}
The non-member functions enable programmers to write code that can be
compiled as either C or \Cpp{}, for example in a shared header file.
\end{note}

\rSec1[atomics.flag]{Flag type and operations}

\begin{codeblock}
namespace std {
  struct atomic_flag {
    constexpr atomic_flag() noexcept;
    atomic_flag(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) = delete;
    atomic_flag& operator=(const atomic_flag&) volatile = delete;

    bool test(memory_order = memory_order::seq_cst) const volatile noexcept;
    bool test(memory_order = memory_order::seq_cst) const noexcept;
    bool test_and_set(memory_order = memory_order::seq_cst) volatile noexcept;
    bool test_and_set(memory_order = memory_order::seq_cst) noexcept;
    void clear(memory_order = memory_order::seq_cst) volatile noexcept;
    void clear(memory_order = memory_order::seq_cst) noexcept;

    void wait(bool, memory_order = memory_order::seq_cst) const volatile noexcept;
    void wait(bool, memory_order = memory_order::seq_cst) const noexcept;
    void notify_one() volatile noexcept;
    void notify_one() noexcept;
    void notify_all() volatile noexcept;
    void notify_all() noexcept;
  };
}
\end{codeblock}

\pnum
The \tcode{atomic_flag} type provides the classic test-and-set functionality. It has two states, set and clear.

\pnum
Operations on an object of type \tcode{atomic_flag} shall be lock-free.
The operations should also be address-free.

\pnum
The \tcode{atomic_flag} type is a standard-layout struct.
It has a trivial destructor.

\indexlibraryctor{atomic_flag}%
\begin{itemdecl}
constexpr atomic_flag::atomic_flag() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Initializes \tcode{*this} to the clear state.
\end{itemdescr}

\indexlibraryglobal{atomic_flag_test}%
\indexlibraryglobal{atomic_flag_test_explicit}%
\indexlibrarymember{test}{atomic_flag}%
\begin{itemdecl}
bool atomic_flag_test(const volatile atomic_flag* object) noexcept;
bool atomic_flag_test(const atomic_flag* object) noexcept;
bool atomic_flag_test_explicit(const volatile atomic_flag* object,
                               memory_order order) noexcept;
bool atomic_flag_test_explicit(const atomic_flag* object,
                               memory_order order) noexcept;
bool atomic_flag::test(memory_order order = memory_order::seq_cst) const volatile noexcept;
bool atomic_flag::test(memory_order order = memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
For \tcode{atomic_flag_test}, let \tcode{order} be \tcode{memory_order::seq_cst}.

\pnum
\expects
\tcode{order} is
neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Memory is affected according to the value of \tcode{order}.

\pnum
\returns
Atomically returns the value pointed to by \tcode{object} or \keyword{this}.
\end{itemdescr}

\indexlibraryglobal{atomic_flag_test_and_set}%
\indexlibraryglobal{atomic_flag_test_and_set_explicit}%
\indexlibrarymember{test_and_set}{atomic_flag}%
\begin{itemdecl}
bool atomic_flag_test_and_set(volatile atomic_flag* object) noexcept;
bool atomic_flag_test_and_set(atomic_flag* object) noexcept;
bool atomic_flag_test_and_set_explicit(volatile atomic_flag* object, memory_order order) noexcept;
bool atomic_flag_test_and_set_explicit(atomic_flag* object, memory_order order) noexcept;
bool atomic_flag::test_and_set(memory_order order = memory_order::seq_cst) volatile noexcept;
bool atomic_flag::test_and_set(memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Atomically sets the value pointed to by \tcode{object} or by \keyword{this} to \tcode{true}. Memory is affected according to the value of
\tcode{order}. These operations are atomic read-modify-write operations\iref{intro.multithread}.

\pnum
\returns
Atomically, the value of the object immediately before the effects.
\end{itemdescr}

\indexlibraryglobal{atomic_flag_clear}%
\indexlibraryglobal{atomic_flag_clear_explicit}%
\indexlibrarymember{clear}{atomic_flag}%
\begin{itemdecl}
void atomic_flag_clear(volatile atomic_flag* object) noexcept;
void atomic_flag_clear(atomic_flag* object) noexcept;
void atomic_flag_clear_explicit(volatile atomic_flag* object, memory_order order) noexcept;
void atomic_flag_clear_explicit(atomic_flag* object, memory_order order) noexcept;
void atomic_flag::clear(memory_order order = memory_order::seq_cst) volatile noexcept;
void atomic_flag::clear(memory_order order = memory_order::seq_cst) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\expects
The \tcode{order} argument is neither \tcode{memory_order::consume},
\tcode{memory_order::acquire}, nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Atomically sets the value pointed to by \tcode{object} or by \keyword{this} to
\tcode{false}. Memory is affected according to the value of \tcode{order}.
\end{itemdescr}

\indexlibraryglobal{atomic_flag_wait}%
\indexlibraryglobal{atomic_flag_wait_explicit}%
\indexlibrarymember{wait}{atomic_flag}%
\begin{itemdecl}
void atomic_flag_wait(const volatile atomic_flag* object, bool old) noexcept;
void atomic_flag_wait(const atomic_flag* object, bool old) noexcept;
void atomic_flag_wait_explicit(const volatile atomic_flag* object,
                               bool old, memory_order order) noexcept;
void atomic_flag_wait_explicit(const atomic_flag* object,
                               bool old, memory_order order) noexcept;
void atomic_flag::wait(bool old, memory_order order =
                                   memory_order::seq_cst) const volatile noexcept;
void atomic_flag::wait(bool old, memory_order order =
                                   memory_order::seq_cst) const noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
For \tcode{atomic_flag_wait},
let \tcode{order} be \tcode{memory_order::seq_cst}.
Let \tcode{flag} be \tcode{object} for the non-member functions and
\keyword{this} for the member functions.

\pnum
\expects
\tcode{order} is
neither \tcode{memory_order::release} nor \tcode{memory_order::acq_rel}.

\pnum
\effects
Repeatedly performs the following steps, in order:
\begin{itemize}
\item
  Evaluates \tcode{flag->test(order) != old}.
\item
  If the result of that evaluation is \tcode{true}, returns.
\item
  Blocks until it
  is unblocked by an atomic notifying operation or is unblocked spuriously.
\end{itemize}

\pnum
\remarks
This function is an atomic waiting operation\iref{atomics.wait}.
\end{itemdescr}

\begin{itemdecl}
void atomic_flag_notify_one(volatile atomic_flag* object) noexcept;
void atomic_flag_notify_one(atomic_flag* object) noexcept;
void atomic_flag::notify_one() volatile noexcept;
void atomic_flag::notify_one() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of at least one atomic waiting operation
that is eligible to be unblocked\iref{atomics.wait} by this call,
if any such atomic waiting operations exist.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\begin{itemdecl}
void atomic_flag_notify_all(volatile atomic_flag* object) noexcept;
void atomic_flag_notify_all(atomic_flag* object) noexcept;
void atomic_flag::notify_all() volatile noexcept;
void atomic_flag::notify_all() noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Unblocks the execution of all atomic waiting operations
that are eligible to be unblocked\iref{atomics.wait} by this call.

\pnum
\remarks
This function is an atomic notifying operation\iref{atomics.wait}.
\end{itemdescr}

\rSec1[atomics.fences]{Fences}

\pnum
This subclause introduces synchronization primitives called \term{fences}. Fences can have
acquire semantics, release semantics, or both. A fence with acquire semantics is called
an \term{acquire fence}. A fence with release semantics is called a \term{release
fence}.

\pnum
A release fence $A$ synchronizes with an acquire fence $B$ if there exist
atomic operations $X$ and $Y$, both operating on some atomic object
$M$, such that $A$ is sequenced before $X$, $X$ modifies
$M$, $Y$ is sequenced before $B$, and $Y$ reads the value
written by $X$ or a value written by any side effect in the hypothetical release
sequence $X$ would head if it were a release operation.

\pnum
A release fence $A$ synchronizes with an atomic operation $B$ that
performs an acquire operation on an atomic object $M$ if there exists an atomic
operation $X$ such that $A$ is sequenced before $X$, $X$
modifies $M$, and $B$ reads the value written by $X$ or a value
written by any side effect in the hypothetical release sequence $X$ would head if
it were a release operation.

\pnum
An atomic operation $A$ that is a release operation on an atomic object
$M$ synchronizes with an acquire fence $B$ if there exists some atomic
operation $X$ on $M$ such that $X$ is sequenced before $B$
and reads the value written by $A$ or a value written by any side effect in the
release sequence headed by $A$.

\indexlibraryglobal{atomic_thread_fence}%
\begin{itemdecl}
extern "C" void atomic_thread_fence(memory_order order) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Depending on the value of \tcode{order}, this operation:
\begin{itemize}
\item has no effects, if \tcode{order == memory_order::relaxed};

\item is an acquire fence, if \tcode{order == memory_order::acquire} or \tcode{order == memory_order::consume};

\item is a release fence, if \tcode{order == memory_order::release};

\item is both an acquire fence and a release fence, if \tcode{order == memory_order::acq_rel};

\item is a sequentially consistent acquire and release fence, if \tcode{order == memory_order::seq_cst}.
\end{itemize}
\end{itemdescr}

\indexlibraryglobal{atomic_signal_fence}%
\begin{itemdecl}
extern "C" void atomic_signal_fence(memory_order order) noexcept;
\end{itemdecl}

\begin{itemdescr}
\pnum
\effects
Equivalent to \tcode{atomic_thread_fence(order)}, except that
the resulting ordering constraints are established only between a thread and a
signal handler executed in the same thread.

\pnum
\begin{note}
\tcode{atomic_signal_fence} can be used to specify the order in which actions
performed by the thread become visible to the signal handler.
Compiler optimizations and reorderings of loads and stores are inhibited in
the same way as with \tcode{atomic_thread_fence}, but the hardware fence instructions
that \tcode{atomic_thread_fence} would have inserted are not emitted.
\end{note}
\end{itemdescr}

\rSec1[stdatomic.h.syn]{C compatibility}

The header \libheaderdef{stdatomic.h} provides the following definitions:

\begin{codeblock}
template<class T>
  using @\exposid{std-atomic}@ = std::atomic<T>;        // exposition only

#define _Atomic(T) @\exposid{std-atomic}@<T>

#define ATOMIC_BOOL_LOCK_FREE @\seebelow@
#define ATOMIC_CHAR_LOCK_FREE @\seebelow@
#define ATOMIC_CHAR16_T_LOCK_FREE @\seebelow@
#define ATOMIC_CHAR32_T_LOCK_FREE @\seebelow@
#define ATOMIC_WCHAR_T_LOCK_FREE @\seebelow@
#define ATOMIC_SHORT_LOCK_FREE @\seebelow@
#define ATOMIC_INT_LOCK_FREE @\seebelow@
#define ATOMIC_LONG_LOCK_FREE @\seebelow@
#define ATOMIC_LLONG_LOCK_FREE @\seebelow@
#define ATOMIC_POINTER_LOCK_FREE @\seebelow@

using std::@\libglobal{memory_order}@;             // \seebelow
using std::@\libglobal{memory_order_relaxed}@;     // \seebelow
using std::@\libglobal{memory_order_consume}@;     // \seebelow
using std::@\libglobal{memory_order_acquire}@;     // \seebelow
using std::@\libglobal{memory_order_release}@;     // \seebelow
using std::@\libglobal{memory_order_acq_rel}@;     // \seebelow
using std::@\libglobal{memory_order_seq_cst}@;     // \seebelow

using std::@\libglobal{atomic_flag}@;              // \seebelow

using std::@\libglobal{atomic_bool}@;              // \seebelow
using std::@\libglobal{atomic_char}@;              // \seebelow
using std::@\libglobal{atomic_schar}@;             // \seebelow
using std::@\libglobal{atomic_uchar}@;             // \seebelow
using std::@\libglobal{atomic_short}@;             // \seebelow
using std::@\libglobal{atomic_ushort}@;            // \seebelow
using std::@\libglobal{atomic_int}@;               // \seebelow
using std::@\libglobal{atomic_uint}@;              // \seebelow
using std::@\libglobal{atomic_long}@;              // \seebelow
using std::@\libglobal{atomic_ulong}@;             // \seebelow
using std::@\libglobal{atomic_llong}@;             // \seebelow
using std::@\libglobal{atomic_ullong}@;            // \seebelow
using std::@\libglobal{atomic_char8_t}@;           // \seebelow
using std::@\libglobal{atomic_char16_t}@;          // \seebelow
using std::@\libglobal{atomic_char32_t}@;          // \seebelow
using std::@\libglobal{atomic_wchar_t}@;           // \seebelow
using std::@\libglobal{atomic_int8_t}@;            // \seebelow
using std::@\libglobal{atomic_uint8_t}@;           // \seebelow
using std::@\libglobal{atomic_int16_t}@;           // \seebelow
using std::@\libglobal{atomic_uint16_t}@;          // \seebelow
using std::@\libglobal{atomic_int32_t}@;           // \seebelow
using std::@\libglobal{atomic_uint32_t}@;          // \seebelow
using std::@\libglobal{atomic_int64_t}@;           // \seebelow
using std::@\libglobal{atomic_uint64_t}@;          // \seebelow
using std::@\libglobal{atomic_int_least8_t}@;      // \seebelow
using std::@\libglobal{atomic_uint_least8_t}@;     // \seebelow
using std::@\libglobal{atomic_int_least16_t}@;     // \seebelow
using std::@\libglobal{atomic_uint_least16_t}@;    // \seebelow
using std::@\libglobal{atomic_int_least32_t}@;     // \seebelow
using std::@\libglobal{atomic_uint_least32_t}@;    // \seebelow
using std::@\libglobal{atomic_int_least64_t}@;     // \seebelow
using std::@\libglobal{atomic_uint_least64_t}@;    // \seebelow
using std::@\libglobal{atomic_int_fast8_t}@;       // \seebelow
using std::@\libglobal{atomic_uint_fast8_t}@;      // \seebelow
using std::@\libglobal{atomic_int_fast16_t}@;      // \seebelow
using std::@\libglobal{atomic_uint_fast16_t}@;     // \seebelow
using std::@\libglobal{atomic_int_fast32_t}@;      // \seebelow
using std::@\libglobal{atomic_uint_fast32_t}@;     // \seebelow
using std::@\libglobal{atomic_int_fast64_t}@;      // \seebelow
using std::@\libglobal{atomic_uint_fast64_t}@;     // \seebelow
using std::@\libglobal{atomic_intptr_t}@;          // \seebelow
using std::@\libglobal{atomic_uintptr_t}@;         // \seebelow
using std::@\libglobal{atomic_size_t}@;            // \seebelow
using std::@\libglobal{atomic_ptrdiff_t}@;         // \seebelow
using std::@\libglobal{atomic_intmax_t}@;          // \seebelow
using std::@\libglobal{atomic_uintmax_t}@;         // \seebelow

using std::@\libglobal{atomic_is_lock_free}@;                          // \seebelow
using std::@\libglobal{atomic_load}@;                                  // \seebelow
using std::@\libglobal{atomic_load_explicit}@;                         // \seebelow
using std::@\libglobal{atomic_store}@;                                 // \seebelow
using std::@\libglobal{atomic_store_explicit}@;                        // \seebelow
using std::@\libglobal{atomic_exchange}@;                              // \seebelow
using std::@\libglobal{atomic_exchange_explicit}@;                     // \seebelow
using std::@\libglobal{atomic_compare_exchange_strong}@;               // \seebelow
using std::@\libglobal{atomic_compare_exchange_strong_explicit}@;      // \seebelow
using std::@\libglobal{atomic_compare_exchange_weak}@;                 // \seebelow
using std::@\libglobal{atomic_compare_exchange_weak_explicit}@;        // \seebelow
using std::@\libglobal{atomic_fetch_add}@;                             // \seebelow
using std::@\libglobal{atomic_fetch_add_explicit}@;                    // \seebelow
using std::@\libglobal{atomic_fetch_sub}@;                             // \seebelow
using std::@\libglobal{atomic_fetch_sub_explicit}@;                    // \seebelow
using std::@\libglobal{atomic_fetch_or}@;                              // \seebelow
using std::@\libglobal{atomic_fetch_or_explicit}@;                     // \seebelow
using std::@\libglobal{atomic_fetch_and}@;                             // \seebelow
using std::@\libglobal{atomic_fetch_and_explicit}@;                    // \seebelow
using std::@\libglobal{atomic_flag_test_and_set}@;                     // \seebelow
using std::@\libglobal{atomic_flag_test_and_set_explicit}@;            // \seebelow
using std::@\libglobal{atomic_flag_clear}@;                            // \seebelow
using std::@\libglobal{atomic_flag_clear_explicit}@;                   // \seebelow

using std::@\libglobal{atomic_thread_fence}@;                          // \seebelow
using std::@\libglobal{atomic_signal_fence}@;                          // \seebelow
\end{codeblock}

\pnum
Each \grammarterm{using-declaration} for some name $A$ in the synopsis above
makes available the same entity as \tcode{std::$A$}
declared in \libheaderrefx{atomic}{atomics.syn}.
Each macro listed above other than \tcode{_Atomic(T)}
is defined as in \libheader{atomic}.
It is unspecified whether \libheader{stdatomic.h} makes available
any declarations in namespace \tcode{std}.

\pnum
Each of the \grammarterm{using-declaration}s for
\tcode{int$N$_t}, \tcode{uint$N$_t}, \tcode{intptr_t}, and \tcode{uintptr_t}
listed above is defined if and only if the implementation defines
the corresponding typedef name in \ref{atomics.syn}.

\pnum
Neither the \tcode{_Atomic} macro,
nor any of the non-macro global namespace declarations,
are provided by any \Cpp{} standard library header
other than \libheader{stdatomic.h}.

\pnum
\recommended
Implementations should ensure
that C and \Cpp{} representations of atomic objects are compatible,
so that the same object can be accessed as both an \tcode{_Atomic(T)}
from C code and an \tcode{atomic<T>} from \Cpp{} code.
The representations should be the same, and
the mechanisms used to ensure atomicity and memory ordering
should be compatible.
